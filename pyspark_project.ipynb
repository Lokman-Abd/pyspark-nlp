{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "w-2thpnybJFt",
    "outputId": "1ec9ff6d-2953-416d-cad8-e1f005d2c020"
   },
   "outputs": [],
   "source": [
    "# !apt-get update # Update apt-get repository.\n",
    "# !apt-get install openjdk-8-jdk-headless -qq > /dev/null # Install Java.\n",
    "# !wget -q http://archive.apache.org/dist/spark/spark-3.1.1/spark-3.1.1-bin-hadoop3.2.tgz # Download Apache Sparks.\n",
    "# !tar xf spark-3.1.1-bin-hadoop3.2.tgz # Unzip the tgz file.\n",
    "# !pip install -q findspark # Install findspark. Adds PySpark to the System path during runtime.\n",
    "\n",
    "# # Set environment variables\n",
    "# import os\n",
    "# os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
    "# os.environ[\"SPARK_HOME\"] = \"/content/spark-3.1.1-bin-hadoop3.2\"\n",
    "\n",
    "# !ls\n",
    "\n",
    "# # Initialize findspark\n",
    "# import findspark\n",
    "# findspark.init()\n",
    "\n",
    "# # Create a PySpark session\n",
    "# from pyspark.sql import SparkSession\n",
    "# spark = SparkSession.builder.master(\"local[*]\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "TbAju8JcbvTh",
    "outputId": "ddbee5f5-5c10-4987-bfa6-1672f789b9f8"
   },
   "outputs": [],
   "source": [
    "# !pip install nltk\n",
    "# !pip install emot\n",
    "# !pip install langdetect\n",
    "# !pip install translators\n",
    "# !pip install pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2024-05-08T07:22:36.226334Z",
     "iopub.status.busy": "2024-05-08T07:22:36.225790Z",
     "iopub.status.idle": "2024-05-08T07:22:42.690690Z",
     "shell.execute_reply": "2024-05-08T07:22:42.689788Z",
     "shell.execute_reply.started": "2024-05-08T07:22:36.226298Z"
    },
    "id": "uMtDch2jb1u5",
    "outputId": "467219ef-b8ff-4255-d81f-50f5b42c7baa"
   },
   "outputs": [],
   "source": [
    "# Importing necessary libraries\n",
    "import numpy as np   # NumPy for numerical operations\n",
    "# import pyspark.pandas as pd # Pandas for data manipulation\n",
    "import nltk\n",
    "# nltk.download('stopwords')\n",
    "# nltk.download('punkt')\n",
    "# nltk.download('wordnet')\n",
    "from nltk.tokenize import word_tokenize  # NLTK for natural language processing - tokenization\n",
    "# nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords  # NLTK for stop words\n",
    "from nltk.stem.lancaster import LancasterStemmer  # NLTK for stemming\n",
    "from nltk.stem.wordnet import WordNetLemmatizer  # NLTK for lemmatization\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer  # Scikit-learn for TF-IDF vectorization\n",
    "from sklearn.model_selection import train_test_split  # Scikit-learn for train-test split\n",
    "from bs4 import BeautifulSoup  # BeautifulSoup for HTML parsing\n",
    "\n",
    "\n",
    "# Importing emot library for emotion analysis\n",
    "# !pip install emot\n",
    "import emot\n",
    "\n",
    "# Regular expression library for text processing\n",
    "import re\n",
    "\n",
    "# Language detection library\n",
    "# !pip install langdetect\n",
    "from langdetect import detect\n",
    "\n",
    "# Translators library for language translation\n",
    "# !pip install translators\n",
    "import translators as ts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-08T07:22:42.695618Z",
     "iopub.status.busy": "2024-05-08T07:22:42.692467Z",
     "iopub.status.idle": "2024-05-08T07:22:42.706133Z",
     "shell.execute_reply": "2024-05-08T07:22:42.704902Z",
     "shell.execute_reply.started": "2024-05-08T07:22:42.695569Z"
    }
   },
   "outputs": [],
   "source": [
    "# # Initialize findspark\n",
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-08T07:22:42.708594Z",
     "iopub.status.busy": "2024-05-08T07:22:42.707965Z",
     "iopub.status.idle": "2024-05-08T07:23:19.665010Z",
     "shell.execute_reply": "2024-05-08T07:23:19.663763Z",
     "shell.execute_reply.started": "2024-05-08T07:22:42.708547Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/05/08 08:22:49 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "sc = SparkSession.builder.appName(\"pyspark_test\").master(\"spark://m1:7077\").\\\n",
    "config(\"spark.executor.memory\",\"1g\").\\\n",
    "config(\"spark.executor.cores\", '4').\\\n",
    "config(\"spark.executor.instances\", 3).\\\n",
    "config(\"spark.sql.shuffle.partitions\", 32).\\\n",
    "config(\"spark.driver.memory\", '1g').getOrCreate()\n",
    "\n",
    "spark_df = sc.read.csv('hdfs://m1:8020/data/Comments.csv', header=True).limit(1000000) # Sample taken from kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-08T07:23:19.668476Z",
     "iopub.status.busy": "2024-05-08T07:23:19.667579Z",
     "iopub.status.idle": "2024-05-08T07:23:19.682885Z",
     "shell.execute_reply": "2024-05-08T07:23:19.681348Z",
     "shell.execute_reply.started": "2024-05-08T07:23:19.668435Z"
    }
   },
   "outputs": [],
   "source": [
    "# spark_df = sc.read.csv('hdfs://m1:8020/data/Comments.csv', header=True) # Sample taken from kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-08T07:23:19.685288Z",
     "iopub.status.busy": "2024-05-08T07:23:19.684299Z",
     "iopub.status.idle": "2024-05-08T07:23:19.915709Z",
     "shell.execute_reply": "2024-05-08T07:23:19.914801Z",
     "shell.execute_reply.started": "2024-05-08T07:23:19.685246Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark_df.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-08T07:23:19.923785Z",
     "iopub.status.busy": "2024-05-08T07:23:19.922489Z",
     "iopub.status.idle": "2024-05-08T07:23:21.433218Z",
     "shell.execute_reply": "2024-05-08T07:23:21.429703Z",
     "shell.execute_reply.started": "2024-05-08T07:23:19.923741Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 1:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+---------+----+--------------------+------------+--------------------+\n",
      "|            id|course_id|rate|                date|display_name|             comment|\n",
      "+--------------+---------+----+--------------------+------------+--------------------+\n",
      "|      88962892|  3173036| 1.0|2021-06-29T18:54:...|       Rahul|I think a beginne...|\n",
      "|Not satisfied.|     NULL|NULL|                NULL|        NULL|                NULL|\n",
      "+--------------+---------+----+--------------------+------------+--------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "spark_df.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2024-05-08T07:23:21.436328Z",
     "iopub.status.busy": "2024-05-08T07:23:21.435976Z",
     "iopub.status.idle": "2024-05-08T07:23:21.444604Z",
     "shell.execute_reply": "2024-05-08T07:23:21.443731Z",
     "shell.execute_reply.started": "2024-05-08T07:23:21.436300Z"
    },
    "id": "4CFfwCIGcU-4",
    "outputId": "d8dbe718-e364-41bf-a6b6-f400a3133263"
   },
   "outputs": [],
   "source": [
    "# from pyspark.sql import SparkSession\n",
    "# sc = SparkSession.builder.getOrCreate()\n",
    "# csv_path = \"hdfs://m1:8020/data/Comments.csv\"  # This will read any CSV file in the /sparkdata folder\n",
    "# spark_df = sc.read.csv(csv_path, header=True)\n",
    "# spark_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2024-05-08T07:23:21.448097Z",
     "iopub.status.busy": "2024-05-08T07:23:21.447747Z",
     "iopub.status.idle": "2024-05-08T07:23:30.016162Z",
     "shell.execute_reply": "2024-05-08T07:23:30.006997Z",
     "shell.execute_reply.started": "2024-05-08T07:23:21.448070Z"
    },
    "id": "sG-cMWPomz1T",
    "outputId": "d8f0343b-c28d-4f80-f5d4-feb3249a0e69"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/05/08 08:23:22 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "[Stage 4:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------------------------------------+--------------------+--------------------+--------------------+--------------------+---------------+\n",
      "|summary|                                   id|           course_id|                rate|                date|        display_name|        comment|\n",
      "+-------+-------------------------------------+--------------------+--------------------+--------------------+--------------------+---------------+\n",
      "|  count|                                 1000|                 847|                 804|                 791|                 781|            778|\n",
      "|   mean|                  1.022588199136598E8|  3928900.3350515463|   4.099871134020619|                NULL|                NULL|           NULL|\n",
      "| stddev|                  2.276865998921982E7|   789422.6896250268|  1.3574216484789199|                NULL|                NULL|           NULL|\n",
      "|    min|                      Thank you fo...| - I am an aspiri...| CoreLocation and...|             PMO-CP\"| a pasar a la acc...| en cada módulo|\n",
      "|    25%|                          9.0281876E7|           3175482.0|                 3.5|                NULL|                NULL|           NULL|\n",
      "|    50%|                         1.12762402E8|           4191614.0|                 5.0|                NULL|                NULL|           NULL|\n",
      "|    75%|                         1.19352746E8|           4669368.0|                 5.0|                NULL|                NULL|           NULL|\n",
      "|    max|非常にわかりやすく、自然に受けるこ...|              997458|                 5.0|2022-10-21T14:28:...|                青山|     音频听不清|\n",
      "+-------+-------------------------------------+--------------------+--------------------+--------------------+--------------------+---------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "spark_df.summary().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-08T07:23:30.018536Z",
     "iopub.status.busy": "2024-05-08T07:23:30.017891Z",
     "iopub.status.idle": "2024-05-08T07:23:30.036716Z",
     "shell.execute_reply": "2024-05-08T07:23:30.035743Z",
     "shell.execute_reply.started": "2024-05-08T07:23:30.018498Z"
    },
    "id": "Iw_GrFPFjSlH"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://m1:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>spark://m1:7077</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>pyspark_test</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=spark://m1:7077 appName=pyspark_test>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-08T07:23:30.047484Z",
     "iopub.status.busy": "2024-05-08T07:23:30.046891Z",
     "iopub.status.idle": "2024-05-08T07:23:33.284692Z",
     "shell.execute_reply": "2024-05-08T07:23:33.283497Z",
     "shell.execute_reply.started": "2024-05-08T07:23:30.047452Z"
    },
    "id": "fgMtNy16cmVs"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 5:==============>                                            (1 + 3) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+----+--------------------+---------------+-------------------------------------+\n",
      "|       id|course_id|rate|                date|   display_name|                              comment|\n",
      "+---------+---------+----+--------------------+---------------+-------------------------------------+\n",
      "| 88962892|  3173036| 1.0|2021-06-29T18:54:...|          Rahul|                 I think a beginne...|\n",
      "|125535470|  4913148| 5.0|2022-10-07T11:17:...|          Marlo|                 Aviva is such a n...|\n",
      "| 68767147|  3178386| 3.5|2020-10-19T06:35:...|  Yamila Andrea|                 Muy buena la intr...|\n",
      "|125029758|  3175814| 5.0|2022-09-30T21:13:...|     Jacqueline|                 This course is th...|\n",
      "| 76584052|  3174896| 4.5|2021-01-30T08:45:...|        Anthony|                 I found this cour...|\n",
      "|124129784|  4693438| 1.0|2022-09-20T11:30:...|          Jiang|                 nothing informati...|\n",
      "|121769970|  4693272| 3.5|2022-08-22T11:48:...|        Kenneth|                 Multiple spelling...|\n",
      "| 57260120|  3168632| 5.0|2020-06-02T22:56:...|           Tony|                 Very unique way o...|\n",
      "| 77427106|  3188362| 4.0|2021-02-09T21:19:...|         HIROKO|グルテンフリーのポイントがよくわか...|\n",
      "|103846020|  4164550| 4.5|2022-01-02T09:04:...|           Jess|                 Good Course!  Inf...|\n",
      "| 93524450|  4164836| 5.0|2021-08-21T16:28:...|          Donna|                 Thanks Kate, grea...|\n",
      "|101205468|  4166134| 5.0|2021-11-27T08:08:...|        Matilde|                 Finalmente un cor...|\n",
      "|118193184|  4693624| 5.0|2022-07-07T04:02:...|           Mark|                 halfway thru. ver...|\n",
      "|122554160|  4695130| 4.0|2022-08-31T10:29:...|        Dominik|                 \"It's a pretty go...|\n",
      "|120487126|  4694990| 4.5|2022-08-05T02:27:...|           Uppu|                 It was very nice ...|\n",
      "|110347774|  4165910| 5.0|2022-03-24T08:56:...|       Zekaryah|                 It is the best co...|\n",
      "|115061440|  4695172| 5.0|2022-05-27T15:52:...|         Vishal|                 I have watched tw...|\n",
      "|124201132|  4161858| 5.0|2022-09-21T06:06:...|      Marenilde|                 Já fiz dois curso...|\n",
      "|114764808|  4694460| 5.0|2022-05-24T10:47:...|Vanlalchhanhima|                 All about Radio A...|\n",
      "| 94540482|  4163248| 3.0|2021-09-02T21:56:...|    Prateek Raj|                 Should have provi...|\n",
      "+---------+---------+----+--------------------+---------------+-------------------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Drop rows with null values in the 'comment' column\n",
    "# spark_df=spark_df.na.drop(subset=['comment'])\n",
    "from pyspark.sql.functions import col\n",
    "# Filter out rows where the \"comment\" column is empty, contains \"nan\", or is null\n",
    "spark_df = spark_df.filter((col(\"comment\") != \"\") \n",
    "                    & (~col(\"comment\").isNull()) \n",
    "                    & (col(\"comment\") != \"nan\")\n",
    "                    & (col(\"rate\") !=\"\")\n",
    "                    & (col(\"rate\").cast(\"int\").isNotNull()))\n",
    "\n",
    "# Show the resulting DataFrame\n",
    "spark_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-08T07:23:33.286665Z",
     "iopub.status.busy": "2024-05-08T07:23:33.285964Z",
     "iopub.status.idle": "2024-05-08T07:23:33.514916Z",
     "shell.execute_reply": "2024-05-08T07:23:33.513327Z",
     "shell.execute_reply.started": "2024-05-08T07:23:33.286624Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[id: string, course_id: string, rate: string, date: string, display_name: string, comment: string]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark_df.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2024-05-08T07:23:33.519714Z",
     "iopub.status.busy": "2024-05-08T07:23:33.519113Z",
     "iopub.status.idle": "2024-05-08T07:23:37.556486Z",
     "shell.execute_reply": "2024-05-08T07:23:37.555567Z",
     "shell.execute_reply.started": "2024-05-08T07:23:33.519667Z"
    },
    "id": "bYXHZX6HoVt3",
    "outputId": "b7942452-cd03-43ba-bc30-6f35878039bb"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of duplicated rows: 0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[id: string, course_id: string, rate: string, date: string, display_name: string, comment: string]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Count the number of duplicated rows\n",
    "dup_count = spark_df.count() - spark_df.dropDuplicates().count()\n",
    "\n",
    "# Show the count of duplicated rows\n",
    "print(\"Number of duplicated rows:\", dup_count)\n",
    "spark_df = spark_df.dropDuplicates()\n",
    "# Delete cache before the next step\n",
    "spark_df.unpersist()\n",
    "spark_df.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2024-05-08T07:23:37.559955Z",
     "iopub.status.busy": "2024-05-08T07:23:37.559429Z",
     "iopub.status.idle": "2024-05-08T07:23:37.587997Z",
     "shell.execute_reply": "2024-05-08T07:23:37.586654Z",
     "shell.execute_reply.started": "2024-05-08T07:23:37.559919Z"
    },
    "id": "-KkKv4ptoyrD",
    "outputId": "5057ccf1-0355-4ae6-eec7-b4486156d55d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['id', 'course_id', 'rate', 'date', 'display_name', 'comment']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2024-05-08T07:23:37.609389Z",
     "iopub.status.busy": "2024-05-08T07:23:37.600694Z",
     "iopub.status.idle": "2024-05-08T07:23:37.661277Z",
     "shell.execute_reply": "2024-05-08T07:23:37.659225Z",
     "shell.execute_reply.started": "2024-05-08T07:23:37.609315Z"
    },
    "id": "cebCvr-DpJSb",
    "outputId": "fcd04da5-c84d-4a54-efe7-e79638637c6b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('id', 'string'),\n",
       " ('course_id', 'string'),\n",
       " ('rate', 'string'),\n",
       " ('date', 'string'),\n",
       " ('display_name', 'string'),\n",
       " ('comment', 'string')]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark_df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-08T07:23:37.673861Z",
     "iopub.status.busy": "2024-05-08T07:23:37.670333Z",
     "iopub.status.idle": "2024-05-08T07:23:38.957126Z",
     "shell.execute_reply": "2024-05-08T07:23:38.953517Z",
     "shell.execute_reply.started": "2024-05-08T07:23:37.673814Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "776"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2024-05-08T07:23:38.975592Z",
     "iopub.status.busy": "2024-05-08T07:23:38.975216Z",
     "iopub.status.idle": "2024-05-08T07:23:43.197620Z",
     "shell.execute_reply": "2024-05-08T07:23:43.196555Z",
     "shell.execute_reply.started": "2024-05-08T07:23:38.975566Z"
    },
    "id": "jsqYzVtXqXFU",
    "outputId": "b7b0518e-552c-4ff0-8133-c7ea320374a1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Null value counts:\n",
      "id: 0\n",
      "course_id: 0\n",
      "rate: 0\n",
      "date: 0\n",
      "display_name: 0\n",
      "comment: 0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Count the number of null values in each column\n",
    "null_counts = [spark_df.where(spark_df[c].isNull()).count() for c in spark_df.columns]\n",
    "\n",
    "# Create a dictionary to store the results\n",
    "null_counts_dict = dict(zip(spark_df.columns, null_counts))\n",
    "\n",
    "# Print the null value counts for each column\n",
    "print(\"Null value counts:\")\n",
    "for column, count in null_counts_dict.items():\n",
    "    print(f\"{column}: {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-08T07:23:43.199240Z",
     "iopub.status.busy": "2024-05-08T07:23:43.198773Z",
     "iopub.status.idle": "2024-05-08T07:23:43.336490Z",
     "shell.execute_reply": "2024-05-08T07:23:43.335501Z",
     "shell.execute_reply.started": "2024-05-08T07:23:43.199202Z"
    },
    "id": "6VDvSpDGeAV9"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[id: string, rate: string, comment: string]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Remove duplicate comments by id\n",
    "spark_df = spark_df.dropDuplicates(['id'])\n",
    "\n",
    "# Drop the 'id', 'display_name', and 'course_id' columns\n",
    "columns_to_drop = [ 'display_name', 'course_id','date']\n",
    "spark_df = spark_df.drop(*columns_to_drop)\n",
    "# Delete cache before the next step\n",
    "spark_df.unpersist()\n",
    "spark_df.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2024-05-08T07:23:43.338233Z",
     "iopub.status.busy": "2024-05-08T07:23:43.337773Z",
     "iopub.status.idle": "2024-05-08T07:23:44.532131Z",
     "shell.execute_reply": "2024-05-08T07:23:44.515391Z",
     "shell.execute_reply.started": "2024-05-08T07:23:43.338202Z"
    },
    "id": "X0a0dtKkunb9",
    "outputId": "6f627c26-4499-4fbc-de09-80e2b5da84ab"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 32:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----+-------------------------------------+\n",
      "|       id|rate|                              comment|\n",
      "+---------+----+-------------------------------------+\n",
      "|100041608| 1.0|                 Not sure if the a...|\n",
      "|100395040| 5.0|                 I would love to t...|\n",
      "|100510019| 4.0|                 Un petit peu plus...|\n",
      "|100647144| 5.0|                                Nett!|\n",
      "|100779534| 3.5|                 PURTROPPO NELLA R...|\n",
      "|100840994| 5.0|小太郎シリーズ３コース受講しました...|\n",
      "|100898808| 5.0|                 Olá professor, vo...|\n",
      "|100912186| 4.0|                 The nuances may h...|\n",
      "|101010552| 5.0|                 A great addition ...|\n",
      "|101071688| 1.0|                 A lot of the answ...|\n",
      "|101109204| 4.5|                            Praktisch|\n",
      "|101203926| 5.0|                 Effettivamente mi...|\n",
      "|101204946| 5.0|                 Luca mi hai apert...|\n",
      "|101205468| 5.0|                 Finalmente un cor...|\n",
      "|101221274| 2.5|                 very basic andnot...|\n",
      "|101365700| 1.5|                 Les questions ne ...|\n",
      "|101407606| 4.0|                 it would have bee...|\n",
      "|101486640| 3.0|                 It would be benef...|\n",
      "|101514696| 5.0|                 A really good qui...|\n",
      "|101568836| 4.5|                        tout est beau|\n",
      "+---------+----+-------------------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "spark_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-08T07:23:44.533825Z",
     "iopub.status.busy": "2024-05-08T07:23:44.533361Z",
     "iopub.status.idle": "2024-05-08T07:23:44.546609Z",
     "shell.execute_reply": "2024-05-08T07:23:44.544169Z",
     "shell.execute_reply.started": "2024-05-08T07:23:44.533791Z"
    },
    "id": "K_b-f5ZIeLpO"
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import StringType\n",
    "from pyspark.sql.types import BooleanType\n",
    "from langdetect import detect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-08T07:23:44.550082Z",
     "iopub.status.busy": "2024-05-08T07:23:44.549586Z",
     "iopub.status.idle": "2024-05-08T07:23:44.820146Z",
     "shell.execute_reply": "2024-05-08T07:23:44.818394Z",
     "shell.execute_reply.started": "2024-05-08T07:23:44.550037Z"
    },
    "id": "vIvu1y2VeA9k"
   },
   "outputs": [],
   "source": [
    "# Function to determine if a comment contains only numbers, blanks, or special characters\n",
    "def contains_only_numbers_or_special_chars(comment):\n",
    "    # Remove special characters and spaces\n",
    "    comment=str(comment)\n",
    "    cleaned_text = re.sub(r'\\W+', '',comment)\n",
    "    # Remove special numbers\n",
    "    cleaned_text =re.sub(r'\\d+', '', cleaned_text)\n",
    "    return cleaned_text.strip()==''\n",
    "contains_only_numbers_or_special_chars_udf = udf(contains_only_numbers_or_special_chars, BooleanType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-08T07:23:44.821931Z",
     "iopub.status.busy": "2024-05-08T07:23:44.821226Z",
     "iopub.status.idle": "2024-05-08T07:23:44.841766Z",
     "shell.execute_reply": "2024-05-08T07:23:44.837948Z",
     "shell.execute_reply.started": "2024-05-08T07:23:44.821856Z"
    },
    "id": "jj6zKZALeQRg"
   },
   "outputs": [],
   "source": [
    "# Function to detect language (similar to the previous example)\n",
    "def detect_language(comment):\n",
    "    try:\n",
    "        return detect(str(comment))\n",
    "    except:\n",
    "        return 'undetermined'  # Handle cases where language detection fails\n",
    "detect_language_udf = udf(detect_language, StringType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-08T07:23:44.845363Z",
     "iopub.status.busy": "2024-05-08T07:23:44.844763Z",
     "iopub.status.idle": "2024-05-08T07:23:45.298103Z",
     "shell.execute_reply": "2024-05-08T07:23:45.288681Z",
     "shell.execute_reply.started": "2024-05-08T07:23:44.845326Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[id: string, rate: string, comment: string, detected_language: string]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark_df = spark_df.withColumn('detected_language', detect_language_udf(spark_df['comment']))\n",
    "# Delete cache before the next step\n",
    "spark_df.unpersist()\n",
    "spark_df.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-08T07:23:45.300977Z",
     "iopub.status.busy": "2024-05-08T07:23:45.299643Z",
     "iopub.status.idle": "2024-05-08T07:23:57.391750Z",
     "shell.execute_reply": "2024-05-08T07:23:57.389065Z",
     "shell.execute_reply.started": "2024-05-08T07:23:45.300936Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 36:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----+-------------------------------------+-----------------+\n",
      "|       id|rate|                              comment|detected_language|\n",
      "+---------+----+-------------------------------------+-----------------+\n",
      "|100041608| 1.0|                 Not sure if the a...|               en|\n",
      "|100395040| 5.0|                 I would love to t...|               en|\n",
      "|100510019| 4.0|                 Un petit peu plus...|               fr|\n",
      "|100647144| 5.0|                                Nett!|               no|\n",
      "|100779534| 3.5|                 PURTROPPO NELLA R...|               pt|\n",
      "|100840994| 5.0|小太郎シリーズ３コース受講しました...|               ja|\n",
      "|100898808| 5.0|                 Olá professor, vo...|               pt|\n",
      "|100912186| 4.0|                 The nuances may h...|               en|\n",
      "|101010552| 5.0|                 A great addition ...|               en|\n",
      "|101071688| 1.0|                 A lot of the answ...|               en|\n",
      "|101109204| 4.5|                            Praktisch|               de|\n",
      "|101203926| 5.0|                 Effettivamente mi...|               it|\n",
      "|101204946| 5.0|                 Luca mi hai apert...|               it|\n",
      "|101205468| 5.0|                 Finalmente un cor...|               es|\n",
      "|101221274| 2.5|                 very basic andnot...|               en|\n",
      "|101365700| 1.5|                 Les questions ne ...|               fr|\n",
      "|101407606| 4.0|                 it would have bee...|               en|\n",
      "|101486640| 3.0|                 It would be benef...|               en|\n",
      "|101514696| 5.0|                 A really good qui...|               en|\n",
      "|101568836| 4.5|                        tout est beau|               fr|\n",
      "+---------+----+-------------------------------------+-----------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "spark_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-08T07:23:57.393502Z",
     "iopub.status.busy": "2024-05-08T07:23:57.392983Z",
     "iopub.status.idle": "2024-05-08T07:23:57.550485Z",
     "shell.execute_reply": "2024-05-08T07:23:57.549076Z",
     "shell.execute_reply.started": "2024-05-08T07:23:57.393463Z"
    },
    "id": "FlixojPXeYrR"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[id: string, rate: string, comment: string, detected_language: string]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "spark_df = spark_df.filter(~contains_only_numbers_or_special_chars_udf(spark_df[\"comment\"]))\n",
    "# Delete cache before the next step\n",
    "spark_df.unpersist()\n",
    "spark_df.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2024-05-08T07:23:57.553581Z",
     "iopub.status.busy": "2024-05-08T07:23:57.552970Z",
     "iopub.status.idle": "2024-05-08T07:23:58.588545Z",
     "shell.execute_reply": "2024-05-08T07:23:58.587202Z",
     "shell.execute_reply.started": "2024-05-08T07:23:57.553552Z"
    },
    "id": "hxlZI-mFm1Qj",
    "outputId": "0a5222f1-e34f-4eaa-9b71-8278a10c7e4c"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 40:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----+-------------------------------------+-----------------+\n",
      "|       id|rate|                              comment|detected_language|\n",
      "+---------+----+-------------------------------------+-----------------+\n",
      "|100041608| 1.0|                 Not sure if the a...|               en|\n",
      "|100395040| 5.0|                 I would love to t...|               en|\n",
      "|100510019| 4.0|                 Un petit peu plus...|               fr|\n",
      "|100647144| 5.0|                                Nett!|               no|\n",
      "|100779534| 3.5|                 PURTROPPO NELLA R...|               pt|\n",
      "|100840994| 5.0|小太郎シリーズ３コース受講しました...|               ja|\n",
      "|100898808| 5.0|                 Olá professor, vo...|               pt|\n",
      "|100912186| 4.0|                 The nuances may h...|               en|\n",
      "|101010552| 5.0|                 A great addition ...|               en|\n",
      "|101071688| 1.0|                 A lot of the answ...|               en|\n",
      "|101109204| 4.5|                            Praktisch|               de|\n",
      "|101203926| 5.0|                 Effettivamente mi...|               it|\n",
      "|101204946| 5.0|                 Luca mi hai apert...|               it|\n",
      "|101205468| 5.0|                 Finalmente un cor...|               es|\n",
      "|101221274| 2.5|                 very basic andnot...|               en|\n",
      "|101365700| 1.5|                 Les questions ne ...|               fr|\n",
      "|101407606| 4.0|                 it would have bee...|               en|\n",
      "|101486640| 3.0|                 It would be benef...|               en|\n",
      "|101514696| 5.0|                 A really good qui...|               en|\n",
      "|101568836| 4.5|                        tout est beau|               fr|\n",
      "+---------+----+-------------------------------------+-----------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "spark_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2024-05-08T07:23:58.594843Z",
     "iopub.status.busy": "2024-05-08T07:23:58.593019Z",
     "iopub.status.idle": "2024-05-08T07:23:59.930204Z",
     "shell.execute_reply": "2024-05-08T07:23:59.926476Z",
     "shell.execute_reply.started": "2024-05-08T07:23:58.594788Z"
    },
    "id": "P2Ly9vYosUa8",
    "outputId": "47f26993-d513-4ea5-bf74-cdbc689575e7"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# how many language are not recognized\n",
    "spark_df.filter(spark_df.detected_language == \"undetermined\").count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-08T07:23:59.945837Z",
     "iopub.status.busy": "2024-05-08T07:23:59.936088Z",
     "iopub.status.idle": "2024-05-08T07:24:00.129520Z",
     "shell.execute_reply": "2024-05-08T07:24:00.128392Z",
     "shell.execute_reply.started": "2024-05-08T07:23:59.945560Z"
    },
    "id": "pwBx0IMWwn0l"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[id: string, rate: string, comment: string, detected_language: string]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#  do not transalte english comments\n",
    "# non_english_comments=df[(df['detected_language'] != 'en')]\n",
    "non_english_comments=spark_df.filter(spark_df.detected_language != \"en\")\n",
    "# Delete cache before the next step\n",
    "non_english_comments.unpersist()\n",
    "non_english_comments.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-08T07:24:00.137369Z",
     "iopub.status.busy": "2024-05-08T07:24:00.136786Z",
     "iopub.status.idle": "2024-05-08T07:24:00.266359Z",
     "shell.execute_reply": "2024-05-08T07:24:00.264929Z",
     "shell.execute_reply.started": "2024-05-08T07:24:00.137336Z"
    },
    "id": "Anf6boHbwp6l"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[id: string, rate: string, comment: string, detected_language: string, cleaned_comment: string]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Function to translate comments\n",
    "def translate_comment(comment):\n",
    "    try:\n",
    "        translated_text = ts.translate_text(comment)\n",
    "    except Exception:\n",
    "        translated_text = \"Error in translation: \"\n",
    "    return translated_text\n",
    "\n",
    "# Register UDF\n",
    "translate_comment_udf = udf(translate_comment, StringType())\n",
    "\n",
    "# Add new column 'cleaned_comment' to the DataFrame\n",
    "non_english_comments = non_english_comments.withColumn('cleaned_comment', translate_comment_udf(spark_df['comment']))\n",
    "# Delete cache before the next step\n",
    "non_english_comments.unpersist()\n",
    "non_english_comments.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2024-05-08T07:24:00.268744Z",
     "iopub.status.busy": "2024-05-08T07:24:00.268081Z",
     "iopub.status.idle": "2024-05-08T07:26:00.632751Z",
     "shell.execute_reply": "2024-05-08T07:26:00.631689Z",
     "shell.execute_reply.started": "2024-05-08T07:24:00.268706Z"
    },
    "id": "8zs6dTCOx0yn",
    "outputId": "b9176a4f-ea6f-491e-c8b8-3e378d70be8f"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 51:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----+-------------------------------------+-----------------+--------------------+\n",
      "|       id|rate|                              comment|detected_language|     cleaned_comment|\n",
      "+---------+----+-------------------------------------+-----------------+--------------------+\n",
      "|100510019| 4.0|                 Un petit peu plus...|               fr|A little more ske...|\n",
      "|100647144| 5.0|                                Nett!|               no|               Nice!|\n",
      "|100779534| 3.5|                 PURTROPPO NELLA R...|               pt|UNFORTUNATELY IN ...|\n",
      "|100840994| 5.0|小太郎シリーズ３コース受講しました...|               ja|I took 3 courses ...|\n",
      "|100898808| 5.0|                 Olá professor, vo...|               pt|Hello professor, ...|\n",
      "|101109204| 4.5|                            Praktisch|               de|           Practical|\n",
      "|101203926| 5.0|                 Effettivamente mi...|               it|Actually, you hav...|\n",
      "|101204946| 5.0|                 Luca mi hai apert...|               it|Luca, you've open...|\n",
      "|101205468| 5.0|                 Finalmente un cor...|               es|Finally a valid c...|\n",
      "|101365700| 1.5|                 Les questions ne ...|               fr|The questions are...|\n",
      "|101568836| 4.5|                        tout est beau|               fr|Everything is bea...|\n",
      "|101576148| 2.0|                 los ultimos 2 vid...|               es|The last 2 videos...|\n",
      "|101642134| 4.0|           説明が丁寧でよかったてす！|               ja|I'm glad the expl...|\n",
      "|101735064| 4.0|                 Курс для детей. Э...|               ru|A course for chil...|\n",
      "|101776502| 5.0|                 Все очень понрави...|               ru|I liked everythin...|\n",
      "|102196590| 3.5|                 Anlatım daha akıc...|               tr|It would be bette...|\n",
      "|102288470| 5.0|とても分かりやすくて、短時間で基礎...|               ja|It was very easy ...|\n",
      "|102872716| 5.0|                 mendapat ilmu leb...|               id|Get more knowledg...|\n",
      "|103011820| 4.0|                 Der Kurs ist stru...|               de|The course is rea...|\n",
      "|103173484| 1.0|                 Nullissime, parle...|               fr|Utterly useless, ...|\n",
      "+---------+----+-------------------------------------+-----------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "non_english_comments.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-08T07:26:00.634246Z",
     "iopub.status.busy": "2024-05-08T07:26:00.633767Z",
     "iopub.status.idle": "2024-05-08T07:26:00.753248Z",
     "shell.execute_reply": "2024-05-08T07:26:00.751547Z",
     "shell.execute_reply.started": "2024-05-08T07:26:00.634207Z"
    },
    "id": "IKhOBKQvGVst"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[id: string, rate: string, comment: string, detected_language: string, cleaned_comment: string]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark_df=spark_df.withColumn(\"cleaned_comment\", spark_df[\"comment\"])\n",
    "# Delete cache before the next step\n",
    "spark_df.unpersist()\n",
    "spark_df.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-08T07:26:00.766361Z",
     "iopub.status.busy": "2024-05-08T07:26:00.755800Z",
     "iopub.status.idle": "2024-05-08T07:26:01.234652Z",
     "shell.execute_reply": "2024-05-08T07:26:01.233206Z",
     "shell.execute_reply.started": "2024-05-08T07:26:00.766315Z"
    },
    "id": "Iy49o4JLH18I"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[id: string, rate: string, comment: string, detected_language: string, cleaned_comment: string]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark_df=non_english_comments.union(spark_df).dropDuplicates(['id'])\n",
    "# Delete cache before the next step\n",
    "spark_df.unpersist()\n",
    "spark_df.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2024-05-08T07:26:01.236518Z",
     "iopub.status.busy": "2024-05-08T07:26:01.235995Z",
     "iopub.status.idle": "2024-05-08T07:26:03.680333Z",
     "shell.execute_reply": "2024-05-08T07:26:03.679540Z",
     "shell.execute_reply.started": "2024-05-08T07:26:01.236488Z"
    },
    "id": "iijCWIJ3vJAk",
    "outputId": "bf7a82e4-1743-4f64-9a3b-6881a51ed675"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 60:======================================================> (31 + 1) / 32]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----+-------------------------------------+-----------------+--------------------+\n",
      "|       id|rate|                              comment|detected_language|     cleaned_comment|\n",
      "+---------+----+-------------------------------------+-----------------+--------------------+\n",
      "|101407606| 4.0|                 it would have bee...|               en|it would have bee...|\n",
      "|104751696| 5.0|                       Sí, muy buena.|               es|     Yes, very good.|\n",
      "|107166928| 5.0|                 Excellent beginne...|               en|Excellent beginne...|\n",
      "|108180506| 4.5|                 Hasta este moment...|               es|So far it's good ...|\n",
      "|115959558| 5.0|                      Amazing company|               tl|     Amazing company|\n",
      "|116593840| 5.0|自分の強みを見つける方法を解説して...|               ja|He explains how t...|\n",
      "|118087848| 1.5|                 kurs jest za szyb...|               pl|The course is bei...|\n",
      "|120121662| 3.0|                 J'aime bien ce fo...|               fr|I like this train...|\n",
      "|124129784| 1.0|                 nothing informati...|               en|nothing informati...|\n",
      "|124201132| 5.0|                 Já fiz dois curso...|               pt|I have already ta...|\n",
      "|125059468| 3.0|                 Dosyć ciekawy kur...|               pl|Quite an interest...|\n",
      "|126063186| 3.5|                 Thank you! Very g...|               en|Thank you! Very g...|\n",
      "|126300040| 5.0|                 otimo direto e ra...|               it|Great, direct and...|\n",
      "|126577120| 1.0|                                  bad|               so|                 bad|\n",
      "|  5392304| 5.0|                     take the course.|               en|    take the course.|\n",
      "| 69283524| 2.0|                 These practice te...|               en|These practice te...|\n",
      "| 70849687| 5.0|                 Clear and concise...|               en|Clear and concise...|\n",
      "| 71190080| 5.0|                 It is a great and...|               en|It is a great and...|\n",
      "| 71277152| 1.0|                 Worst, please don...|               en|Worst, please don...|\n",
      "| 86649386| 4.0|                 I did learn some ...|               en|I did learn some ...|\n",
      "+---------+----+-------------------------------------+-----------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "spark_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-08T07:26:36.425851Z",
     "iopub.status.busy": "2024-05-08T07:26:36.425014Z",
     "iopub.status.idle": "2024-05-08T07:26:36.432101Z",
     "shell.execute_reply": "2024-05-08T07:26:36.430578Z",
     "shell.execute_reply.started": "2024-05-08T07:26:36.425802Z"
    }
   },
   "outputs": [],
   "source": [
    "# spark_df.write.csv(\"hdfs://m1:8020/output2.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-08T07:26:37.415749Z",
     "iopub.status.busy": "2024-05-08T07:26:37.414796Z",
     "iopub.status.idle": "2024-05-08T07:26:37.517095Z",
     "shell.execute_reply": "2024-05-08T07:26:37.515732Z",
     "shell.execute_reply.started": "2024-05-08T07:26:37.415709Z"
    },
    "id": "7U8185MvU5Kn"
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import when\n",
    "# Set sentiment based on rate\n",
    "spark_df = spark_df.withColumn('sentiment',\n",
    "                   when(spark_df['rate'] < 2, 'negative')\n",
    "                   .when((spark_df['rate'] >= 2) & (spark_df['rate'] <= 3), 'neutral')\n",
    "                   .otherwise('positive'))\n",
    "spark_df = spark_df.withColumn('label',\n",
    "                   when(spark_df['rate'] < 2, 0)\n",
    "                   .when((spark_df['rate'] >= 2) & (spark_df['rate'] <= 3), 50)\n",
    "                   .otherwise(99))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2024-05-08T07:26:38.270239Z",
     "iopub.status.busy": "2024-05-08T07:26:38.269318Z",
     "iopub.status.idle": "2024-05-08T07:26:39.891554Z",
     "shell.execute_reply": "2024-05-08T07:26:39.890500Z",
     "shell.execute_reply.started": "2024-05-08T07:26:38.270196Z"
    },
    "id": "6Ul-ysebw8V-",
    "outputId": "a89f2c90-95a0-4390-83aa-a026b5686d5e"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 66:==========================================>             (24 + 4) / 32]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----+\n",
      "|sentiment|count|\n",
      "+---------+-----+\n",
      "|  neutral|  110|\n",
      "| positive|  574|\n",
      "| negative|   90|\n",
      "+---------+-----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "spark_df.groupBy(\"sentiment\").count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-08T07:26:39.893631Z",
     "iopub.status.busy": "2024-05-08T07:26:39.893210Z",
     "iopub.status.idle": "2024-05-08T07:26:40.513645Z",
     "shell.execute_reply": "2024-05-08T07:26:40.512646Z",
     "shell.execute_reply.started": "2024-05-08T07:26:39.893599Z"
    },
    "id": "nHShoeSVVB-s"
   },
   "outputs": [],
   "source": [
    "#  convert emoji and emoticon into words\n",
    "emot_obj = emot.core.emot()\n",
    "def replace_emojis_with_meanings(text):\n",
    "    emojis = emot_obj.emoji(text)['value']\n",
    "    emojis_meanings =  emot_obj.emoji(text)['mean']\n",
    "    emoticons = emot_obj.emoticons(text)['value']\n",
    "    emoticons_meanings =  emot_obj.emoticons(text)['mean']\n",
    "\n",
    "    # Remove special characters from meanings list\n",
    "    emojis_meanings = [re.sub(r'[^\\w\\s]', '', meaning) for meaning in emojis_meanings]\n",
    "     # Remove special characters from meanings list\n",
    "    emoticons_meanings = [re.sub(r'[^\\w\\s]', '', meaning) for meaning in emoticons_meanings]\n",
    "\n",
    "    # Replace emojis in the text with the corresponding meanings\n",
    "    for emoji, meaning in zip(emojis, emojis_meanings):\n",
    "        text = text.replace(emoji, meaning)\n",
    "\n",
    "     # Replace emojis in the text with the corresponding meanings\n",
    "    for emoji, meaning in zip(emoticons, emoticons_meanings):\n",
    "        text = text.replace(emoji, meaning)\n",
    "\n",
    "    return text\n",
    "replace_emojis_with_meanings_udf = udf(replace_emojis_with_meanings, StringType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-08T07:26:40.515634Z",
     "iopub.status.busy": "2024-05-08T07:26:40.515126Z",
     "iopub.status.idle": "2024-05-08T07:26:40.523770Z",
     "shell.execute_reply": "2024-05-08T07:26:40.521064Z",
     "shell.execute_reply.started": "2024-05-08T07:26:40.515593Z"
    },
    "id": "a5mH63pWVEG2"
   },
   "outputs": [],
   "source": [
    "# Function to remove links using regular expressions\n",
    "def remove_links(comment):\n",
    "    # Regular expression pattern to match URLs\n",
    "    pattern = r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\\\(\\\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+'\n",
    "    return re.sub(pattern, '', comment)\n",
    "remove_links_udf = udf(remove_links, StringType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-08T07:26:40.612641Z",
     "iopub.status.busy": "2024-05-08T07:26:40.612291Z",
     "iopub.status.idle": "2024-05-08T07:26:40.618482Z",
     "shell.execute_reply": "2024-05-08T07:26:40.617396Z",
     "shell.execute_reply.started": "2024-05-08T07:26:40.612617Z"
    },
    "id": "1-lF-J-QWBfE"
   },
   "outputs": [],
   "source": [
    "# Function to remove HTML tags\n",
    "def remove_html_tags(text):\n",
    "    soup = BeautifulSoup(text, \"html.parser\")\n",
    "    # Get the text without HTML tags\n",
    "    text_without_tags = soup.get_text(separator=\" \", strip=True)\n",
    "    return text_without_tags\n",
    "remove_html_tags_udf = udf(remove_html_tags, StringType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-08T07:26:41.264172Z",
     "iopub.status.busy": "2024-05-08T07:26:41.263151Z",
     "iopub.status.idle": "2024-05-08T07:26:41.272668Z",
     "shell.execute_reply": "2024-05-08T07:26:41.270415Z",
     "shell.execute_reply.started": "2024-05-08T07:26:41.264142Z"
    },
    "id": "y3zxaF6PWIrh"
   },
   "outputs": [],
   "source": [
    "# Function to remove_numbers_or_special_chars\n",
    "def remove_numbers_or_special_chars(comment):\n",
    "     # Replace escape sequences with an empty string\n",
    "    cleaned_text = re.sub(r'\\\\[^\\s]', ' ', comment)\n",
    "    # convert n't into not EX(didn't => did not)\n",
    "    cleaned_text= cleaned_text.replace(\"n't\", \" not\")\n",
    "    cleaned_text= cleaned_text.replace(\"_\", \" \")\n",
    "    # Remove special characters and spaces\n",
    "    cleaned_text = re.sub(r'\\W+', ' ',cleaned_text)\n",
    "    # Remove special numbers\n",
    "    cleaned_text =re.sub(r'\\d+', ' ', cleaned_text)\n",
    "    return cleaned_text\n",
    "remove_numbers_or_special_chars_udf = udf(remove_html_tags, StringType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-08T07:26:41.876400Z",
     "iopub.status.busy": "2024-05-08T07:26:41.875374Z",
     "iopub.status.idle": "2024-05-08T07:26:41.885448Z",
     "shell.execute_reply": "2024-05-08T07:26:41.883357Z",
     "shell.execute_reply.started": "2024-05-08T07:26:41.876361Z"
    },
    "id": "DxpmLtCyWPsa"
   },
   "outputs": [],
   "source": [
    "# apply cleaning functions on comments\n",
    "def clean_text(comment):\n",
    "    text_res = replace_emojis_with_meanings(comment)\n",
    "    text_res = text_res.lower()\n",
    "    text_res = remove_links(text_res)\n",
    "    text_res = remove_html_tags(text_res)\n",
    "    text_res = remove_numbers_or_special_chars(text_res)\n",
    "    return text_res\n",
    "clean_text_udf = udf(clean_text, StringType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2024-05-08T07:26:42.748650Z",
     "iopub.status.busy": "2024-05-08T07:26:42.747966Z",
     "iopub.status.idle": "2024-05-08T07:26:43.363075Z",
     "shell.execute_reply": "2024-05-08T07:26:43.360932Z",
     "shell.execute_reply.started": "2024-05-08T07:26:42.748609Z"
    },
    "id": "v7L00kDaxVdH",
    "outputId": "1769d059-eb1a-4517-fcb4-9bc61424662f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----+-------------------------------------+-----------------+--------------------+---------+-----+\n",
      "|       id|rate|                              comment|detected_language|     cleaned_comment|sentiment|label|\n",
      "+---------+----+-------------------------------------+-----------------+--------------------+---------+-----+\n",
      "|101407606| 4.0|                 it would have bee...|               en|it would have bee...| positive|   99|\n",
      "|104751696| 5.0|                       Sí, muy buena.|               es|     Yes, very good.| positive|   99|\n",
      "|107166928| 5.0|                 Excellent beginne...|               en|Excellent beginne...| positive|   99|\n",
      "|108180506| 4.5|                 Hasta este moment...|               es|So far it's good ...| positive|   99|\n",
      "|115959558| 5.0|                      Amazing company|               tl|     Amazing company| positive|   99|\n",
      "|116593840| 5.0|自分の強みを見つける方法を解説して...|               ja|He explains how t...| positive|   99|\n",
      "|118087848| 1.5|                 kurs jest za szyb...|               pl|The course is bei...| negative|    0|\n",
      "|120121662| 3.0|                 J'aime bien ce fo...|               fr|I like this train...|  neutral|   50|\n",
      "|124129784| 1.0|                 nothing informati...|               en|nothing informati...| negative|    0|\n",
      "|124201132| 5.0|                 Já fiz dois curso...|               pt|I have already ta...| positive|   99|\n",
      "|125059468| 3.0|                 Dosyć ciekawy kur...|               pl|Quite an interest...|  neutral|   50|\n",
      "|126063186| 3.5|                 Thank you! Very g...|               en|Thank you! Very g...|  neutral|   50|\n",
      "|126300040| 5.0|                 otimo direto e ra...|               it|Great, direct and...| positive|   99|\n",
      "|126577120| 1.0|                                  bad|               so|                 bad| negative|    0|\n",
      "|  5392304| 5.0|                     take the course.|               en|    take the course.| positive|   99|\n",
      "| 69283524| 2.0|                 These practice te...|               en|These practice te...|  neutral|   50|\n",
      "| 70849687| 5.0|                 Clear and concise...|               en|Clear and concise...| positive|   99|\n",
      "| 71190080| 5.0|                 It is a great and...|               en|It is a great and...| positive|   99|\n",
      "| 71277152| 1.0|                 Worst, please don...|               en|Worst, please don...| negative|    0|\n",
      "| 86649386| 4.0|                 I did learn some ...|               en|I did learn some ...| positive|   99|\n",
      "+---------+----+-------------------------------------+-----------------+--------------------+---------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-08T07:26:44.159104Z",
     "iopub.status.busy": "2024-05-08T07:26:44.158289Z",
     "iopub.status.idle": "2024-05-08T07:26:44.257269Z",
     "shell.execute_reply": "2024-05-08T07:26:44.255123Z",
     "shell.execute_reply.started": "2024-05-08T07:26:44.159059Z"
    },
    "id": "D26aKgH9WXdP"
   },
   "outputs": [],
   "source": [
    "# Apply the clean_text function to the 'cleaned_comment' column\n",
    "spark_df = spark_df.withColumn('cleaned_comment', clean_text_udf(spark_df['cleaned_comment']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-08T07:26:45.229191Z",
     "iopub.status.busy": "2024-05-08T07:26:45.228290Z",
     "iopub.status.idle": "2024-05-08T07:26:45.249111Z",
     "shell.execute_reply": "2024-05-08T07:26:45.247883Z",
     "shell.execute_reply.started": "2024-05-08T07:26:45.229150Z"
    },
    "id": "66jtPwG1XTOH"
   },
   "outputs": [],
   "source": [
    "# remove stop words and lemmatization\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "# Words to keep\n",
    "\n",
    "words_to_keep = {'very','too','so','not','no','but'}\n",
    "# keep words like \"very\", and \"so\"\n",
    "stop_words = {word for word in stop_words if  word not in words_to_keep}\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def preprocess_text(comment):\n",
    "      tokens = word_tokenize(comment.lower())  # Tokenization\n",
    "      tokens = [token for token in tokens if token.isalpha()]  # Remove non-alphabetic tokens\n",
    "      tokens = [token for token in tokens if token not in stop_words]  # Remove stop words\n",
    "      tokens = [lemmatizer.lemmatize(token) for token in tokens]  # Lemmatization\n",
    "      return ' '.join(tokens)\n",
    "preprocess_text_udf = udf(preprocess_text, StringType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-08T07:26:47.877119Z",
     "iopub.status.busy": "2024-05-08T07:26:47.875777Z",
     "iopub.status.idle": "2024-05-08T07:26:47.937101Z",
     "shell.execute_reply": "2024-05-08T07:26:47.935985Z",
     "shell.execute_reply.started": "2024-05-08T07:26:47.877067Z"
    },
    "id": "7MHQ0RcwXVl9"
   },
   "outputs": [],
   "source": [
    "spark_df = spark_df.withColumn('cleaned_text', preprocess_text_udf(spark_df['cleaned_comment']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2024-05-08T07:26:48.882662Z",
     "iopub.status.busy": "2024-05-08T07:26:48.881444Z",
     "iopub.status.idle": "2024-05-08T07:26:55.779830Z",
     "shell.execute_reply": "2024-05-08T07:26:55.778703Z",
     "shell.execute_reply.started": "2024-05-08T07:26:48.882615Z"
    },
    "id": "EyZP2VjBv-Qo",
    "outputId": "091300be-d3b4-4b70-ae54-7825fb2e8d29"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 76:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----+-------------------------------------+-----------------+--------------------+---------+-----+--------------------+\n",
      "|       id|rate|                              comment|detected_language|     cleaned_comment|sentiment|label|        cleaned_text|\n",
      "+---------+----+-------------------------------------+-----------------+--------------------+---------+-----+--------------------+\n",
      "|101407606| 4.0|                 it would have bee...|               en|it would have bee...| positive|   99|would lot better ...|\n",
      "|104751696| 5.0|                       Sí, muy buena.|               es|      yes very good | positive|   99|       yes very good|\n",
      "|107166928| 5.0|                 Excellent beginne...|               en|excellent beginne...| positive|   99|excellent beginne...|\n",
      "|108180506| 4.5|                 Hasta este moment...|               es|so far it s good ...| positive|   99|so far good also ...|\n",
      "|115959558| 5.0|                      Amazing company|               tl|     amazing company| positive|   99|     amazing company|\n",
      "|116593840| 5.0|自分の強みを見つける方法を解説して...|               ja|he explains how t...| positive|   99|explains find str...|\n",
      "|118087848| 1.5|                 kurs jest za szyb...|               pl|the course is bei...| negative|    0|course run too fa...|\n",
      "|120121662| 3.0|                 J'aime bien ce fo...|               fr|i like this train...|  neutral|   50|like trainer but ...|\n",
      "|124129784| 1.0|                 nothing informati...|               en|nothing informati...| negative|    0| nothing information|\n",
      "|124201132| 5.0|                 Já fiz dois curso...|               pt|i have already ta...| positive|   99|already taken two...|\n",
      "|125059468| 3.0|                 Dosyć ciekawy kur...|               pl|quite an interest...|  neutral|   50|quite interesting...|\n",
      "|126063186| 3.5|                 Thank you! Very g...|               en|thank you very go...|  neutral|   50|thank very good i...|\n",
      "|126300040| 5.0|                 otimo direto e ra...|               it|great direct and ...| positive|   99|   great direct fast|\n",
      "|126577120| 1.0|                                  bad|               so|                 bad| negative|    0|                 bad|\n",
      "|  5392304| 5.0|                     take the course.|               en|    take the course | positive|   99|         take course|\n",
      "| 69283524| 2.0|                 These practice te...|               en|these practice te...|  neutral|   50|practice test val...|\n",
      "| 70849687| 5.0|                 Clear and concise...|               en|clear and concise...| positive|   99|clear concise exa...|\n",
      "| 71190080| 5.0|                 It is a great and...|               en|it is a great and...| positive|   99|great easy explan...|\n",
      "| 71277152| 1.0|                 Worst, please don...|               en|worst please do n...| negative|    0|worst please not ...|\n",
      "| 86649386| 4.0|                 I did learn some ...|               en|i did learn some ...| positive|   99|learn thing seeme...|\n",
      "+---------+----+-------------------------------------+-----------------+--------------------+---------+-----+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "spark_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-08T08:07:05.075041Z",
     "iopub.status.busy": "2024-05-08T08:07:05.073777Z",
     "iopub.status.idle": "2024-05-08T08:07:05.213543Z",
     "shell.execute_reply": "2024-05-08T08:07:05.212157Z",
     "shell.execute_reply.started": "2024-05-08T08:07:05.075002Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['id',\n",
       " 'rate',\n",
       " 'comment',\n",
       " 'detected_language',\n",
       " 'cleaned_comment',\n",
       " 'sentiment',\n",
       " 'label',\n",
       " 'cleaned_text']"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-08T08:58:44.555691Z",
     "iopub.status.busy": "2024-05-08T08:58:44.551917Z",
     "iopub.status.idle": "2024-05-08T08:58:44.667269Z",
     "shell.execute_reply": "2024-05-08T08:58:44.665939Z",
     "shell.execute_reply.started": "2024-05-08T08:58:44.553880Z"
    }
   },
   "outputs": [],
   "source": [
    "from couchdb import Server\n",
    "\n",
    "\n",
    "# Create a CouchDB Server object\n",
    "server = Server(\"http://172.26.0.5:5984/\")\n",
    "\n",
    "# Authenticate with CouchDB (if required)\n",
    "server.resource.credentials = (\"admin\", \"admin\")\n",
    "\n",
    "# Create or get the database\n",
    "db_name = \"nlp_db\"\n",
    "db = server.create(db_name)\n",
    "\n",
    "# Loop over each row in the DataFrame and save to CouchDB\n",
    "for row in spark_df.collect():\n",
    "    doc = {\n",
    "        \"_id\": row.id,\n",
    "        \"rate\": row.rate,\n",
    "        \"comment\": row.comment,\n",
    "        \"detected_language\": row.detected_language,\n",
    "        \"cleaned_comment\": row.cleaned_comment,\n",
    "        \"sentiment\": row.sentiment,\n",
    "        \"label\": row.label,\n",
    "        \"cleaned_text\": row.cleaned_text\n",
    "    }\n",
    "    db.save_doc(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2024-05-08T07:26:55.781555Z",
     "iopub.status.busy": "2024-05-08T07:26:55.781124Z",
     "iopub.status.idle": "2024-05-08T07:26:55.794997Z",
     "shell.execute_reply": "2024-05-08T07:26:55.788109Z",
     "shell.execute_reply.started": "2024-05-08T07:26:55.781520Z"
    },
    "id": "bsT89PtnFm1d",
    "outputId": "cf6092f6-a25a-4af7-86ab-d48e6fa3ef6e"
   },
   "outputs": [],
   "source": [
    "# # Define a dictionary to map sentiment values to label values\n",
    "# sentiment_label_map = {\"positive\": 1, \"neutral\": 0, \"negative\": -1}\n",
    "\n",
    "# # Loop over sentiment values and set new column called 'label'\n",
    "# for sentiment, label in sentiment_label_map.items():\n",
    "#     spark_df = spark_df.withColumn(\"label\", when(spark_df[\"sentiment\"] == sentiment, label))\n",
    "# spark_df.show(spark_df.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-08T07:27:20.183076Z",
     "iopub.status.busy": "2024-05-08T07:27:20.182218Z",
     "iopub.status.idle": "2024-05-08T07:28:30.818470Z",
     "shell.execute_reply": "2024-05-08T07:28:30.817426Z",
     "shell.execute_reply.started": "2024-05-08T07:27:20.183044Z"
    },
    "id": "qLiSB4_Dzy9S"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import Tokenizer, StopWordsRemover, CountVectorizer, IDF, StringIndexer\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml import Pipeline\n",
    "# Sample data\n",
    "# Replace `spark_df` with your actual Spark DataFrame containing 'cleaned_text' and 'sentiment' columns\n",
    "# Assuming 'sentiment' is your target column and 'cleaned_text' is your feature column\n",
    "# Replace `test_ratio` with your desired test set ratio\n",
    "test_ratio = 0.2\n",
    "\n",
    "# Split the data into train and test sets\n",
    "train_data, test_data = spark_df.randomSplit([1 - test_ratio, test_ratio], seed=42)\n",
    "\n",
    "# Define the stages of the pipeline\n",
    "tokenizer = Tokenizer(inputCol=\"cleaned_text\", outputCol=\"words\")\n",
    "count_vectorizer = CountVectorizer(inputCol=\"words\", outputCol=\"rawFeatures\")\n",
    "idf = IDF(inputCol=\"rawFeatures\", outputCol=\"features\")\n",
    "# Define the classifier model\n",
    "rf = RandomForestClassifier(featuresCol=\"features\", labelCol=\"label\", seed=42)\n",
    "\n",
    "# Define the pipeline\n",
    "pipeline = Pipeline(stages=[ tokenizer,count_vectorizer, idf, rf])\n",
    "\n",
    "# Train the model\n",
    "model = pipeline.fit(train_data)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2024-05-08T07:28:30.830819Z",
     "iopub.status.busy": "2024-05-08T07:28:30.829743Z",
     "iopub.status.idle": "2024-05-08T07:28:33.869983Z",
     "shell.execute_reply": "2024-05-08T07:28:33.843735Z",
     "shell.execute_reply.started": "2024-05-08T07:28:30.830771Z"
    },
    "id": "J5IVo6H4Ca9J",
    "outputId": "55fff348-8466-4ca0-b3be-426829f5c8eb"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 131:==============>                                          (1 + 3) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------+----------+\n",
      "|        cleaned_text|sentiment|prediction|\n",
      "+--------------------+---------+----------+\n",
      "|excellent beginne...| positive|      99.0|\n",
      "|course run too fa...| negative|      99.0|\n",
      "| nothing information| negative|      99.0|\n",
      "|                 bad| negative|      99.0|\n",
      "|learn thing seeme...| positive|      99.0|\n",
      "|great understandi...| positive|      99.0|\n",
      "|took course kotar...| positive|      99.0|\n",
      "|gone question so ...| negative|      99.0|\n",
      "|very least provid...|  neutral|      99.0|\n",
      "|                good| positive|      99.0|\n",
      "|very disappointed...| negative|      99.0|\n",
      "|due language acce...|  neutral|      99.0|\n",
      "|finally valid cou...| positive|      99.0|\n",
      "|            anything| negative|      99.0|\n",
      "|very practical us...| positive|      99.0|\n",
      "|definitely course...| positive|      99.0|\n",
      "|good knowledgable...| positive|      99.0|\n",
      "|           no so far| negative|      99.0|\n",
      "|found unclear exp...| negative|      99.0|\n",
      "|        great course| positive|      99.0|\n",
      "+--------------------+---------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Make predictions on the test data\n",
    "predictions = model.transform(test_data)\n",
    "\n",
    "# Show some sample predictions\n",
    "predictions.select(\"cleaned_text\", \"sentiment\", \"prediction\").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2024-05-08T07:28:33.876133Z",
     "iopub.status.busy": "2024-05-08T07:28:33.871390Z",
     "iopub.status.idle": "2024-05-08T07:28:43.517150Z",
     "shell.execute_reply": "2024-05-08T07:28:43.515826Z",
     "shell.execute_reply.started": "2024-05-08T07:28:33.876053Z"
    },
    "id": "1TtmHRJoCvqo",
    "outputId": "d052e54e-2df7-44b6-b1b9-3d605c191904"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted sentiment: 99.0\n"
     ]
    }
   ],
   "source": [
    "# prompt: with my model make prodection for this comment \"i really hate that !\"\n",
    "\n",
    "comment = \"i really love that !\"\n",
    "cleaned_comment = clean_text(comment)\n",
    "cleaned_text = preprocess_text(cleaned_comment)\n",
    "data = sc.createDataFrame([(cleaned_text, )], [\"cleaned_text\"])\n",
    "predictions = model.transform(data)\n",
    "predicted_sentiment = predictions.select(\"prediction\").collect()[0][0]\n",
    "print(f\"Predicted sentiment: {predicted_sentiment}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-08T07:28:43.539070Z",
     "iopub.status.busy": "2024-05-08T07:28:43.537759Z",
     "iopub.status.idle": "2024-05-08T07:28:43.560354Z",
     "shell.execute_reply": "2024-05-08T07:28:43.559702Z",
     "shell.execute_reply.started": "2024-05-08T07:28:43.538913Z"
    },
    "id": "Ey3cVNicA_0G"
   },
   "outputs": [],
   "source": [
    "# # Evaluate the model (example: using MulticlassClassificationEvaluator)\n",
    "# from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "# evaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "# accuracy = evaluator.evaluate(predictions)\n",
    "# print(\"Test Accuracy =\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-05-08T07:42:16.711729Z",
     "iopub.status.idle": "2024-05-08T07:42:16.717347Z",
     "shell.execute_reply": "2024-05-08T07:42:16.717001Z",
     "shell.execute_reply.started": "2024-05-08T07:42:16.716961Z"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import HashingTF, IDF, Tokenizer\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.ml.pipeline import Pipeline\n",
    "\n",
    "## Load the data into a Spark DataFrame\n",
    "df = spark_df[[\"cleaned_text\", \"label\"]]\n",
    "\n",
    "## Split the data into training and testing sets\n",
    "(trainDF, testDF) = df.randomSplit([0.8, 0.2], seed=42)\n",
    "\n",
    "## Define a pipeline with Tokenizer, HashingTF, IDF, and Logistic Regression\n",
    "tokenizer = Tokenizer(inputCol=\"cleaned_text\", outputCol=\"words\")\n",
    "hashingTF = HashingTF(inputCol=\"words\", outputCol=\"features\", numFeatures=5000)\n",
    "idf = IDF(inputCol=\"features\", outputCol=\"tfidf_features\")\n",
    "lr = LogisticRegression(featuresCol=\"tfidf_features\", labelCol=\"label\")\n",
    "\n",
    "pipeline = Pipeline(stages=[tokenizer, hashingTF, idf, lr])\n",
    "\n",
    "## Train the model\n",
    "model = pipeline.fit(trainDF)\n",
    "\n",
    "## Make predictions\n",
    "trainPredictions = model.transform(trainDF)\n",
    "testPredictions = model.transform(testDF)\n",
    "\n",
    "## Calculate accuracy\n",
    "trainAccuracy = MulticlassClassificationEvaluator(labelCol=\"lable\", predictionCol=\"prediction\", metricName=\"accuracy\").evaluate(trainPredictions)\n",
    "testAccuracy = MulticlassClassificationEvaluator(labelCol=\"lable\", predictionCol=\"prediction\", metricName=\"accuracy\").evaluate(testPredictions)\n",
    "\n",
    "print(f\"Training Accuracy: {trainAccuracy:.4f}\")\n",
    "print(f\"Testing Accuracy: {testAccuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import HashingTF, IDF, Tokenizer\n",
    "from pyspark.ml.classification import NaiveBayes\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.ml.pipeline import Pipeline\n",
    "\n",
    "## Load the data into a Spark DataFrame\n",
    "\n",
    "\n",
    "## Define a pipeline with Tokenizer, HashingTF, IDF, and Multinomial Naive Bayes\n",
    "tokenizer = Tokenizer(inputCol=\"cleaned_text\", outputCol=\"words\")\n",
    "hashingTF = HashingTF(inputCol=\"words\", outputCol=\"features\", numFeatures=5000)\n",
    "idf = IDF(inputCol=\"features\", outputCol=\"tfidf_features\")\n",
    "nb = NaiveBayes(featuresCol=\"tfidf_features\", labelCol=\"label\")\n",
    "\n",
    "pipeline = Pipeline(stages=[tokenizer, hashingTF, idf, nb])\n",
    "\n",
    "## Train the model\n",
    "model = pipeline.fit(trainDF)\n",
    "\n",
    "## Make predictions\n",
    "trainPredictions = model.transform(trainDF)\n",
    "testPredictions = model.transform(testDF)\n",
    "\n",
    "## Calculate accuracy\n",
    "trainAccuracy = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\").evaluate(trainPredictions)\n",
    "testAccuracy = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\").evaluate(testPredictions)\n",
    "\n",
    "print(f\"Multinomial Naive Bayes Training Accuracy: {trainAccuracy:.4f}\")\n",
    "print(f\"Multinomial Naive Bayes Testing Accuracy: {testAccuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import HashingTF, IDF, Tokenizer\n",
    "from pyspark.ml.classification import SVM\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.ml.pipeline import Pipeline\n",
    "\n",
    "\n",
    "## Define a pipeline with Tokenizer, HashingTF, IDF, and Support Vector Machine\n",
    "tokenizer = Tokenizer(inputCol=\"clean_text\", outputCol=\"words\")\n",
    "hashingTF = HashingTF(inputCol=\"words\", outputCol=\"features\", numFeatures=5000)\n",
    "idf = IDF(inputCol=\"features\", outputCol=\"tfidf_features\")\n",
    "svm = SVM(featuresCol=\"tfidf_features\", labelCol=\"sentiment\", kernel=\"linear\", C=1.0)\n",
    "\n",
    "pipeline = Pipeline(stages=[tokenizer, hashingTF, idf, svm])\n",
    "\n",
    "## Train the model\n",
    "model = pipeline.fit(trainDF)\n",
    "\n",
    "## Make predictions\n",
    "trainPredictions = model.transform(trainDF)\n",
    "testPredictions = model.transform(testDF)\n",
    "\n",
    "## Calculate accuracy\n",
    "trainAccuracy = MulticlassClassificationEvaluator(labelCol=\"sentiment\", predictionCol=\"prediction\", metricName=\"accuracy\").evaluate(trainPredictions)\n",
    "testAccuracy = MulticlassClassificationEvaluator(labelCol=\"sentiment\", predictionCol=\"prediction\", metricName=\"accuracy\").evaluate(testPredictions)\n",
    "\n",
    "print(f\"Accuracy: {testAccuracy:.2f}\")\n",
    "\n",
    "## Generate classification report\n",
    "report = MulticlassClassificationEvaluator(labelCol=\"sentiment\", predictionCol=\"prediction\", metricName=\"f1\").evaluate(testPredictions)\n",
    "print(\"Classification Report:\")\n",
    "print(f\"F1 Score: {report:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

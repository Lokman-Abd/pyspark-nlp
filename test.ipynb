{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2024-05-08T08:51:15.351190Z",
     "iopub.status.busy": "2024-05-08T08:51:15.350347Z",
     "iopub.status.idle": "2024-05-08T08:51:15.356597Z",
     "shell.execute_reply": "2024-05-08T08:51:15.355023Z",
     "shell.execute_reply.started": "2024-05-08T08:51:15.351161Z"
    },
    "id": "w-2thpnybJFt",
    "outputId": "1ec9ff6d-2953-416d-cad8-e1f005d2c020"
   },
   "outputs": [],
   "source": [
    "# !apt-get update # Update apt-get repository.\n",
    "# !apt-get install openjdk-8-jdk-headless -qq > /dev/null # Install Java.\n",
    "# !wget -q http://archive.apache.org/dist/spark/spark-3.1.1/spark-3.1.1-bin-hadoop3.2.tgz # Download Apache Sparks.\n",
    "# !tar xf spark-3.1.1-bin-hadoop3.2.tgz # Unzip the tgz file.\n",
    "# !pip install -q findspark # Install findspark. Adds PySpark to the System path during runtime.\n",
    "\n",
    "# # Set environment variables\n",
    "# import os\n",
    "# os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
    "# os.environ[\"SPARK_HOME\"] = \"/content/spark-3.1.1-bin-hadoop3.2\"\n",
    "\n",
    "# !ls\n",
    "\n",
    "# # Initialize findspark\n",
    "# import findspark\n",
    "# findspark.init()\n",
    "\n",
    "# # Create a PySpark session\n",
    "# from pyspark.sql import SparkSession\n",
    "# spark = SparkSession.builder.master(\"local[*]\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "execution": {
     "iopub.execute_input": "2024-05-08T08:51:15.498504Z",
     "iopub.status.busy": "2024-05-08T08:51:15.498043Z",
     "iopub.status.idle": "2024-05-08T08:51:15.505367Z",
     "shell.execute_reply": "2024-05-08T08:51:15.503482Z",
     "shell.execute_reply.started": "2024-05-08T08:51:15.498477Z"
    },
    "id": "TbAju8JcbvTh",
    "outputId": "ddbee5f5-5c10-4987-bfa6-1672f789b9f8"
   },
   "outputs": [],
   "source": [
    "# !pip install nltk\n",
    "# !pip install emot\n",
    "# !pip install langdetect\n",
    "# !pip install translators\n",
    "# !pip install pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2024-05-08T08:51:15.795379Z",
     "iopub.status.busy": "2024-05-08T08:51:15.794010Z",
     "iopub.status.idle": "2024-05-08T08:51:15.805271Z",
     "shell.execute_reply": "2024-05-08T08:51:15.803130Z",
     "shell.execute_reply.started": "2024-05-08T08:51:15.795330Z"
    },
    "id": "uMtDch2jb1u5",
    "outputId": "467219ef-b8ff-4255-d81f-50f5b42c7baa"
   },
   "outputs": [],
   "source": [
    "# Importing necessary libraries\n",
    "import numpy as np   # NumPy for numerical operations\n",
    "# import pyspark.pandas as pd # Pandas for data manipulation\n",
    "import nltk\n",
    "# nltk.download('stopwords')\n",
    "# nltk.download('punkt')\n",
    "# nltk.download('wordnet')\n",
    "from nltk.tokenize import word_tokenize  # NLTK for natural language processing - tokenization\n",
    "# nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords  # NLTK for stop words\n",
    "from nltk.stem.lancaster import LancasterStemmer  # NLTK for stemming\n",
    "from nltk.stem.wordnet import WordNetLemmatizer  # NLTK for lemmatization\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer  # Scikit-learn for TF-IDF vectorization\n",
    "from sklearn.model_selection import train_test_split  # Scikit-learn for train-test split\n",
    "from bs4 import BeautifulSoup  # BeautifulSoup for HTML parsing\n",
    "\n",
    "\n",
    "# Importing emot library for emotion analysis\n",
    "# !pip install emot\n",
    "import emot\n",
    "\n",
    "# Regular expression library for text processing\n",
    "import re\n",
    "\n",
    "# Language detection library\n",
    "# !pip install langdetect\n",
    "from langdetect import detect\n",
    "\n",
    "# Translators library for language translation\n",
    "# !pip install translators\n",
    "import translators as ts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-08T08:51:16.126643Z",
     "iopub.status.busy": "2024-05-08T08:51:16.125718Z",
     "iopub.status.idle": "2024-05-08T08:51:16.132201Z",
     "shell.execute_reply": "2024-05-08T08:51:16.130696Z",
     "shell.execute_reply.started": "2024-05-08T08:51:16.126607Z"
    }
   },
   "outputs": [],
   "source": [
    "# # Initialize findspark\n",
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-08T08:51:16.541911Z",
     "iopub.status.busy": "2024-05-08T08:51:16.541020Z",
     "iopub.status.idle": "2024-05-08T08:51:29.218987Z",
     "shell.execute_reply": "2024-05-08T08:51:29.218065Z",
     "shell.execute_reply.started": "2024-05-08T08:51:16.541881Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "sc = SparkSession.builder.appName(\"pyspark_test\").master(\"spark://m1:7077\").\\\n",
    "config(\"spark.executor.memory\",\"1g\").\\\n",
    "config(\"spark.executor.cores\", '4').\\\n",
    "config(\"spark.executor.instances\", 4).\\\n",
    "config(\"spark.sql.shuffle.partitions\", 32).\\\n",
    "config(\"spark.driver.memory\", '1g').getOrCreate()\n",
    "\n",
    "spark_df = sc.read.csv('hdfs://m1:8020/data/Comments.csv', header=True).limit(10) # Sample taken from kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-08T08:51:29.221062Z",
     "iopub.status.busy": "2024-05-08T08:51:29.220619Z",
     "iopub.status.idle": "2024-05-08T08:51:29.231168Z",
     "shell.execute_reply": "2024-05-08T08:51:29.229229Z",
     "shell.execute_reply.started": "2024-05-08T08:51:29.221027Z"
    }
   },
   "outputs": [],
   "source": [
    "# spark_df = sc.read.csv('hdfs://m1:8020/data/Comments.csv', header=True) # Sample taken from kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-08T08:51:29.233361Z",
     "iopub.status.busy": "2024-05-08T08:51:29.232581Z",
     "iopub.status.idle": "2024-05-08T08:51:29.464029Z",
     "shell.execute_reply": "2024-05-08T08:51:29.462693Z",
     "shell.execute_reply.started": "2024-05-08T08:51:29.233319Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark_df.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-08T08:51:29.466501Z",
     "iopub.status.busy": "2024-05-08T08:51:29.466035Z",
     "iopub.status.idle": "2024-05-08T08:51:31.207532Z",
     "shell.execute_reply": "2024-05-08T08:51:31.206021Z",
     "shell.execute_reply.started": "2024-05-08T08:51:29.466469Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 1:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+---------+----+--------------------+------------+--------------------+\n",
      "|            id|course_id|rate|                date|display_name|             comment|\n",
      "+--------------+---------+----+--------------------+------------+--------------------+\n",
      "|      88962892|  3173036| 1.0|2021-06-29T18:54:...|       Rahul|I think a beginne...|\n",
      "|Not satisfied.|     NULL|NULL|                NULL|        NULL|                NULL|\n",
      "+--------------+---------+----+--------------------+------------+--------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "spark_df.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2024-05-08T08:51:31.475518Z",
     "iopub.status.busy": "2024-05-08T08:51:31.474937Z",
     "iopub.status.idle": "2024-05-08T08:51:31.483833Z",
     "shell.execute_reply": "2024-05-08T08:51:31.481757Z",
     "shell.execute_reply.started": "2024-05-08T08:51:31.475492Z"
    },
    "id": "4CFfwCIGcU-4",
    "outputId": "d8dbe718-e364-41bf-a6b6-f400a3133263"
   },
   "outputs": [],
   "source": [
    "# from pyspark.sql import SparkSession\n",
    "# sc = SparkSession.builder.getOrCreate()\n",
    "# csv_path = \"hdfs://m1:8020/data/Comments.csv\"  # This will read any CSV file in the /sparkdata folder\n",
    "# spark_df = sc.read.csv(csv_path, header=True)\n",
    "# spark_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2024-05-08T08:51:32.300635Z",
     "iopub.status.busy": "2024-05-08T08:51:32.300107Z",
     "iopub.status.idle": "2024-05-08T08:51:40.253404Z",
     "shell.execute_reply": "2024-05-08T08:51:40.250790Z",
     "shell.execute_reply.started": "2024-05-08T08:51:32.300598Z"
    },
    "id": "sG-cMWPomz1T",
    "outputId": "d8f0343b-c28d-4f80-f5d4-feb3249a0e69"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/05/08 09:51:34 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "[Stage 4:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+------------------+------------------+--------------------+-------------+--------------------+\n",
      "|summary|                  id|         course_id|              rate|                date| display_name|             comment|\n",
      "+-------+--------------------+------------------+------------------+--------------------+-------------+--------------------+\n",
      "|  count|                  10|                 7|                 7|                   7|            7|                   7|\n",
      "|   mean|1.0439701042857143E8|3857427.1428571427| 3.357142857142857|                NULL|         NULL|                NULL|\n",
      "| stddev|2.5314708039155934E7| 853622.7361739746|1.7251638983558855|                NULL|         NULL|                NULL|\n",
      "|    min|           121769970|           3173036|               1.0|2020-10-19T06:35:...|      Anthony|Aviva is such a n...|\n",
      "|    25%|         7.6584052E7|         3174896.0|               1.0|                NULL|         NULL|                NULL|\n",
      "|    50%|         1.2176997E8|         3178386.0|               3.5|                NULL|         NULL|                NULL|\n",
      "|    75%|        1.25029758E8|         4693438.0|               5.0|                NULL|         NULL|                NULL|\n",
      "|    max|También sería muy...|           4913148|               5.0|2022-10-07T11:17:...|Yamila Andrea|nothing informati...|\n",
      "+-------+--------------------+------------------+------------------+--------------------+-------------+--------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "spark_df.summary().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-08T08:51:40.260817Z",
     "iopub.status.busy": "2024-05-08T08:51:40.260368Z",
     "iopub.status.idle": "2024-05-08T08:51:40.275863Z",
     "shell.execute_reply": "2024-05-08T08:51:40.274687Z",
     "shell.execute_reply.started": "2024-05-08T08:51:40.260786Z"
    },
    "id": "Iw_GrFPFjSlH"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://m1:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>spark://m1:7077</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>pyspark_test</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=spark://m1:7077 appName=pyspark_test>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-08T08:51:40.278820Z",
     "iopub.status.busy": "2024-05-08T08:51:40.277791Z",
     "iopub.status.idle": "2024-05-08T08:51:41.958429Z",
     "shell.execute_reply": "2024-05-08T08:51:41.957622Z",
     "shell.execute_reply.started": "2024-05-08T08:51:40.278778Z"
    },
    "id": "fgMtNy16cmVs"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 5:==============>                                            (1 + 3) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+----+--------------------+-------------+--------------------+\n",
      "|       id|course_id|rate|                date| display_name|             comment|\n",
      "+---------+---------+----+--------------------+-------------+--------------------+\n",
      "| 88962892|  3173036| 1.0|2021-06-29T18:54:...|        Rahul|I think a beginne...|\n",
      "|125535470|  4913148| 5.0|2022-10-07T11:17:...|        Marlo|Aviva is such a n...|\n",
      "| 68767147|  3178386| 3.5|2020-10-19T06:35:...|Yamila Andrea|Muy buena la intr...|\n",
      "|125029758|  3175814| 5.0|2022-09-30T21:13:...|   Jacqueline|This course is th...|\n",
      "| 76584052|  3174896| 4.5|2021-01-30T08:45:...|      Anthony|I found this cour...|\n",
      "|124129784|  4693438| 1.0|2022-09-20T11:30:...|        Jiang|nothing informati...|\n",
      "|121769970|  4693272| 3.5|2022-08-22T11:48:...|      Kenneth|Multiple spelling...|\n",
      "+---------+---------+----+--------------------+-------------+--------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Drop rows with null values in the 'comment' column\n",
    "# spark_df=spark_df.na.drop(subset=['comment'])\n",
    "from pyspark.sql.functions import col\n",
    "# Filter out rows where the \"comment\" column is empty, contains \"nan\", or is null\n",
    "spark_df = spark_df.filter((col(\"comment\") != \"\") \n",
    "                    & (~col(\"comment\").isNull()) \n",
    "                    & (col(\"comment\") != \"nan\")\n",
    "                    & (col(\"rate\") !=\"\")\n",
    "                    & (col(\"rate\").cast(\"int\").isNotNull()))\n",
    "\n",
    "# Show the resulting DataFrame\n",
    "spark_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-08T08:51:41.960603Z",
     "iopub.status.busy": "2024-05-08T08:51:41.960286Z",
     "iopub.status.idle": "2024-05-08T08:51:42.438750Z",
     "shell.execute_reply": "2024-05-08T08:51:42.437803Z",
     "shell.execute_reply.started": "2024-05-08T08:51:41.960578Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[id: string, course_id: string, rate: string, date: string, display_name: string, comment: string]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark_df.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2024-05-08T08:51:42.460967Z",
     "iopub.status.busy": "2024-05-08T08:51:42.453143Z",
     "iopub.status.idle": "2024-05-08T08:51:45.027689Z",
     "shell.execute_reply": "2024-05-08T08:51:45.026420Z",
     "shell.execute_reply.started": "2024-05-08T08:51:42.460698Z"
    },
    "id": "bYXHZX6HoVt3",
    "outputId": "b7942452-cd03-43ba-bc30-6f35878039bb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of duplicated rows: 0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[id: string, course_id: string, rate: string, date: string, display_name: string, comment: string]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Count the number of duplicated rows\n",
    "dup_count = spark_df.count() - spark_df.dropDuplicates().count()\n",
    "\n",
    "# Show the count of duplicated rows\n",
    "print(\"Number of duplicated rows:\", dup_count)\n",
    "spark_df = spark_df.dropDuplicates()\n",
    "# Delete cache before the next step\n",
    "spark_df.unpersist()\n",
    "spark_df.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2024-05-08T08:51:45.032416Z",
     "iopub.status.busy": "2024-05-08T08:51:45.032020Z",
     "iopub.status.idle": "2024-05-08T08:51:45.047929Z",
     "shell.execute_reply": "2024-05-08T08:51:45.044987Z",
     "shell.execute_reply.started": "2024-05-08T08:51:45.032386Z"
    },
    "id": "-KkKv4ptoyrD",
    "outputId": "5057ccf1-0355-4ae6-eec7-b4486156d55d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['id', 'course_id', 'rate', 'date', 'display_name', 'comment']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2024-05-08T08:51:45.050593Z",
     "iopub.status.busy": "2024-05-08T08:51:45.049563Z",
     "iopub.status.idle": "2024-05-08T08:51:45.101681Z",
     "shell.execute_reply": "2024-05-08T08:51:45.100846Z",
     "shell.execute_reply.started": "2024-05-08T08:51:45.050550Z"
    },
    "id": "cebCvr-DpJSb",
    "outputId": "fcd04da5-c84d-4a54-efe7-e79638637c6b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('id', 'string'),\n",
       " ('course_id', 'string'),\n",
       " ('rate', 'string'),\n",
       " ('date', 'string'),\n",
       " ('display_name', 'string'),\n",
       " ('comment', 'string')]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark_df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-08T08:51:45.104056Z",
     "iopub.status.busy": "2024-05-08T08:51:45.102615Z",
     "iopub.status.idle": "2024-05-08T08:51:46.515300Z",
     "shell.execute_reply": "2024-05-08T08:51:46.512971Z",
     "shell.execute_reply.started": "2024-05-08T08:51:45.104013Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2024-05-08T08:51:46.519281Z",
     "iopub.status.busy": "2024-05-08T08:51:46.518787Z",
     "iopub.status.idle": "2024-05-08T08:51:48.866571Z",
     "shell.execute_reply": "2024-05-08T08:51:48.865484Z",
     "shell.execute_reply.started": "2024-05-08T08:51:46.519248Z"
    },
    "id": "jsqYzVtXqXFU",
    "outputId": "b7b0518e-552c-4ff0-8133-c7ea320374a1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Null value counts:\n",
      "id: 0\n",
      "course_id: 0\n",
      "rate: 0\n",
      "date: 0\n",
      "display_name: 0\n",
      "comment: 0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Count the number of null values in each column\n",
    "null_counts = [spark_df.where(spark_df[c].isNull()).count() for c in spark_df.columns]\n",
    "\n",
    "# Create a dictionary to store the results\n",
    "null_counts_dict = dict(zip(spark_df.columns, null_counts))\n",
    "\n",
    "# Print the null value counts for each column\n",
    "print(\"Null value counts:\")\n",
    "for column, count in null_counts_dict.items():\n",
    "    print(f\"{column}: {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-08T08:51:48.878143Z",
     "iopub.status.busy": "2024-05-08T08:51:48.876113Z",
     "iopub.status.idle": "2024-05-08T08:51:49.119962Z",
     "shell.execute_reply": "2024-05-08T08:51:49.113852Z",
     "shell.execute_reply.started": "2024-05-08T08:51:48.878102Z"
    },
    "id": "6VDvSpDGeAV9"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[id: string, rate: string, comment: string]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Remove duplicate comments by id\n",
    "spark_df = spark_df.dropDuplicates(['id'])\n",
    "\n",
    "# Drop the 'id', 'display_name', and 'course_id' columns\n",
    "columns_to_drop = [ 'display_name', 'course_id','date']\n",
    "spark_df = spark_df.drop(*columns_to_drop)\n",
    "# Delete cache before the next step\n",
    "spark_df.unpersist()\n",
    "spark_df.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2024-05-08T08:51:49.126856Z",
     "iopub.status.busy": "2024-05-08T08:51:49.126254Z",
     "iopub.status.idle": "2024-05-08T08:51:50.459622Z",
     "shell.execute_reply": "2024-05-08T08:51:50.448704Z",
     "shell.execute_reply.started": "2024-05-08T08:51:49.126818Z"
    },
    "id": "X0a0dtKkunb9",
    "outputId": "6f627c26-4499-4fbc-de09-80e2b5da84ab"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 32:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----+--------------------+\n",
      "|       id|rate|             comment|\n",
      "+---------+----+--------------------+\n",
      "|121769970| 3.5|Multiple spelling...|\n",
      "|124129784| 1.0|nothing informati...|\n",
      "|125029758| 5.0|This course is th...|\n",
      "|125535470| 5.0|Aviva is such a n...|\n",
      "| 68767147| 3.5|Muy buena la intr...|\n",
      "| 76584052| 4.5|I found this cour...|\n",
      "| 88962892| 1.0|I think a beginne...|\n",
      "+---------+----+--------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "spark_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-08T08:51:50.462748Z",
     "iopub.status.busy": "2024-05-08T08:51:50.461905Z",
     "iopub.status.idle": "2024-05-08T08:51:50.500436Z",
     "shell.execute_reply": "2024-05-08T08:51:50.499698Z",
     "shell.execute_reply.started": "2024-05-08T08:51:50.462623Z"
    },
    "id": "K_b-f5ZIeLpO"
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import StringType\n",
    "from pyspark.sql.types import BooleanType\n",
    "from langdetect import detect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-08T08:51:50.501945Z",
     "iopub.status.busy": "2024-05-08T08:51:50.501308Z",
     "iopub.status.idle": "2024-05-08T08:51:50.586942Z",
     "shell.execute_reply": "2024-05-08T08:51:50.585923Z",
     "shell.execute_reply.started": "2024-05-08T08:51:50.501903Z"
    },
    "id": "vIvu1y2VeA9k"
   },
   "outputs": [],
   "source": [
    "# Function to determine if a comment contains only numbers, blanks, or special characters\n",
    "def contains_only_numbers_or_special_chars(comment):\n",
    "    # Remove special characters and spaces\n",
    "    comment=str(comment)\n",
    "    cleaned_text = re.sub(r'\\W+', '',comment)\n",
    "    # Remove special numbers\n",
    "    cleaned_text =re.sub(r'\\d+', '', cleaned_text)\n",
    "    return cleaned_text.strip()==''\n",
    "contains_only_numbers_or_special_chars_udf = udf(contains_only_numbers_or_special_chars, BooleanType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-08T08:51:50.595780Z",
     "iopub.status.busy": "2024-05-08T08:51:50.590739Z",
     "iopub.status.idle": "2024-05-08T08:51:50.627151Z",
     "shell.execute_reply": "2024-05-08T08:51:50.626160Z",
     "shell.execute_reply.started": "2024-05-08T08:51:50.595739Z"
    },
    "id": "jj6zKZALeQRg"
   },
   "outputs": [],
   "source": [
    "# Function to detect language (similar to the previous example)\n",
    "def detect_language(comment):\n",
    "    try:\n",
    "        return detect(str(comment))\n",
    "    except:\n",
    "        return 'undetermined'  # Handle cases where language detection fails\n",
    "detect_language_udf = udf(detect_language, StringType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-08T08:51:50.629151Z",
     "iopub.status.busy": "2024-05-08T08:51:50.628826Z",
     "iopub.status.idle": "2024-05-08T08:51:51.042194Z",
     "shell.execute_reply": "2024-05-08T08:51:51.040444Z",
     "shell.execute_reply.started": "2024-05-08T08:51:50.629128Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[id: string, rate: string, comment: string, detected_language: string]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark_df = spark_df.withColumn('detected_language', detect_language_udf(spark_df['comment']))\n",
    "# Delete cache before the next step\n",
    "spark_df.unpersist()\n",
    "spark_df.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-08T08:51:51.049647Z",
     "iopub.status.busy": "2024-05-08T08:51:51.049238Z",
     "iopub.status.idle": "2024-05-08T08:51:56.223321Z",
     "shell.execute_reply": "2024-05-08T08:51:56.218167Z",
     "shell.execute_reply.started": "2024-05-08T08:51:51.049616Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 36:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----+--------------------+-----------------+\n",
      "|       id|rate|             comment|detected_language|\n",
      "+---------+----+--------------------+-----------------+\n",
      "|121769970| 3.5|Multiple spelling...|               en|\n",
      "|124129784| 1.0|nothing informati...|               en|\n",
      "|125029758| 5.0|This course is th...|               en|\n",
      "|125535470| 5.0|Aviva is such a n...|               en|\n",
      "| 68767147| 3.5|Muy buena la intr...|               es|\n",
      "| 76584052| 4.5|I found this cour...|               en|\n",
      "| 88962892| 1.0|I think a beginne...|               en|\n",
      "+---------+----+--------------------+-----------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "spark_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-08T08:51:56.224882Z",
     "iopub.status.busy": "2024-05-08T08:51:56.224488Z",
     "iopub.status.idle": "2024-05-08T08:51:56.345238Z",
     "shell.execute_reply": "2024-05-08T08:51:56.343804Z",
     "shell.execute_reply.started": "2024-05-08T08:51:56.224853Z"
    },
    "id": "FlixojPXeYrR"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[id: string, rate: string, comment: string, detected_language: string]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "spark_df = spark_df.filter(~contains_only_numbers_or_special_chars_udf(spark_df[\"comment\"]))\n",
    "# Delete cache before the next step\n",
    "spark_df.unpersist()\n",
    "spark_df.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2024-05-08T08:51:56.346886Z",
     "iopub.status.busy": "2024-05-08T08:51:56.346547Z",
     "iopub.status.idle": "2024-05-08T08:51:57.690473Z",
     "shell.execute_reply": "2024-05-08T08:51:57.682947Z",
     "shell.execute_reply.started": "2024-05-08T08:51:56.346860Z"
    },
    "id": "hxlZI-mFm1Qj",
    "outputId": "0a5222f1-e34f-4eaa-9b71-8278a10c7e4c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----+--------------------+-----------------+\n",
      "|       id|rate|             comment|detected_language|\n",
      "+---------+----+--------------------+-----------------+\n",
      "|121769970| 3.5|Multiple spelling...|               en|\n",
      "|124129784| 1.0|nothing informati...|               en|\n",
      "|125029758| 5.0|This course is th...|               en|\n",
      "|125535470| 5.0|Aviva is such a n...|               en|\n",
      "| 68767147| 3.5|Muy buena la intr...|               es|\n",
      "| 76584052| 4.5|I found this cour...|               en|\n",
      "| 88962892| 1.0|I think a beginne...|               en|\n",
      "+---------+----+--------------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2024-05-08T08:51:57.693760Z",
     "iopub.status.busy": "2024-05-08T08:51:57.693186Z",
     "iopub.status.idle": "2024-05-08T08:51:58.161029Z",
     "shell.execute_reply": "2024-05-08T08:51:58.159924Z",
     "shell.execute_reply.started": "2024-05-08T08:51:57.693728Z"
    },
    "id": "P2Ly9vYosUa8",
    "outputId": "47f26993-d513-4ea5-bf74-cdbc689575e7"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# how many language are not recognized\n",
    "spark_df.filter(spark_df.detected_language == \"undetermined\").count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-08T08:51:58.164189Z",
     "iopub.status.busy": "2024-05-08T08:51:58.163706Z",
     "iopub.status.idle": "2024-05-08T08:51:58.224795Z",
     "shell.execute_reply": "2024-05-08T08:51:58.219950Z",
     "shell.execute_reply.started": "2024-05-08T08:51:58.164155Z"
    },
    "id": "pwBx0IMWwn0l"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[id: string, rate: string, comment: string, detected_language: string]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#  do not transalte english comments\n",
    "# non_english_comments=df[(df['detected_language'] != 'en')]\n",
    "non_english_comments=spark_df.filter(spark_df.detected_language != \"en\")\n",
    "# Delete cache before the next step\n",
    "non_english_comments.unpersist()\n",
    "non_english_comments.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-08T08:51:58.229970Z",
     "iopub.status.busy": "2024-05-08T08:51:58.229184Z",
     "iopub.status.idle": "2024-05-08T08:51:58.333225Z",
     "shell.execute_reply": "2024-05-08T08:51:58.331918Z",
     "shell.execute_reply.started": "2024-05-08T08:51:58.229897Z"
    },
    "id": "Anf6boHbwp6l"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[id: string, rate: string, comment: string, detected_language: string, cleaned_comment: string]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Function to translate comments\n",
    "def translate_comment(comment):\n",
    "    try:\n",
    "        translated_text = ts.translate_text(comment)\n",
    "    except Exception:\n",
    "        translated_text = \"Error in translation: \"\n",
    "    return translated_text\n",
    "\n",
    "# Register UDF\n",
    "translate_comment_udf = udf(translate_comment, StringType())\n",
    "\n",
    "# Add new column 'cleaned_comment' to the DataFrame\n",
    "non_english_comments = non_english_comments.withColumn('cleaned_comment', translate_comment_udf(spark_df['comment']))\n",
    "# Delete cache before the next step\n",
    "non_english_comments.unpersist()\n",
    "non_english_comments.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2024-05-08T08:51:58.336291Z",
     "iopub.status.busy": "2024-05-08T08:51:58.335486Z",
     "iopub.status.idle": "2024-05-08T08:52:03.888008Z",
     "shell.execute_reply": "2024-05-08T08:52:03.887040Z",
     "shell.execute_reply.started": "2024-05-08T08:51:58.336259Z"
    },
    "id": "8zs6dTCOx0yn",
    "outputId": "b9176a4f-ea6f-491e-c8b8-3e378d70be8f"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 51:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----+--------------------+-----------------+--------------------+\n",
      "|      id|rate|             comment|detected_language|     cleaned_comment|\n",
      "+--------+----+--------------------+-----------------+--------------------+\n",
      "|68767147| 3.5|Muy buena la intr...|               es|The introduction ...|\n",
      "+--------+----+--------------------+-----------------+--------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "non_english_comments.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-08T08:52:03.889503Z",
     "iopub.status.busy": "2024-05-08T08:52:03.889110Z",
     "iopub.status.idle": "2024-05-08T08:52:03.969152Z",
     "shell.execute_reply": "2024-05-08T08:52:03.967863Z",
     "shell.execute_reply.started": "2024-05-08T08:52:03.889472Z"
    },
    "id": "IKhOBKQvGVst"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[id: string, rate: string, comment: string, detected_language: string, cleaned_comment: string]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark_df=spark_df.withColumn(\"cleaned_comment\", spark_df[\"comment\"])\n",
    "# Delete cache before the next step\n",
    "spark_df.unpersist()\n",
    "spark_df.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-08T08:52:03.972184Z",
     "iopub.status.busy": "2024-05-08T08:52:03.970600Z",
     "iopub.status.idle": "2024-05-08T08:52:04.145368Z",
     "shell.execute_reply": "2024-05-08T08:52:04.139066Z",
     "shell.execute_reply.started": "2024-05-08T08:52:03.972138Z"
    },
    "id": "Iy49o4JLH18I"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[id: string, rate: string, comment: string, detected_language: string, cleaned_comment: string]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark_df=non_english_comments.union(spark_df).dropDuplicates(['id'])\n",
    "# Delete cache before the next step\n",
    "spark_df.unpersist()\n",
    "spark_df.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2024-05-08T08:52:04.147314Z",
     "iopub.status.busy": "2024-05-08T08:52:04.146884Z",
     "iopub.status.idle": "2024-05-08T08:52:06.658636Z",
     "shell.execute_reply": "2024-05-08T08:52:06.655852Z",
     "shell.execute_reply.started": "2024-05-08T08:52:04.147244Z"
    },
    "id": "iijCWIJ3vJAk",
    "outputId": "bf7a82e4-1743-4f64-9a3b-6881a51ed675"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 60:==================================================>     (29 + 3) / 32]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----+--------------------+-----------------+--------------------+\n",
      "|       id|rate|             comment|detected_language|     cleaned_comment|\n",
      "+---------+----+--------------------+-----------------+--------------------+\n",
      "|124129784| 1.0|nothing informati...|               en|nothing informati...|\n",
      "| 88962892| 1.0|I think a beginne...|               en|I think a beginne...|\n",
      "|121769970| 3.5|Multiple spelling...|               en|Multiple spelling...|\n",
      "| 76584052| 4.5|I found this cour...|               en|I found this cour...|\n",
      "|125029758| 5.0|This course is th...|               en|This course is th...|\n",
      "| 68767147| 3.5|Muy buena la intr...|               es|The introduction ...|\n",
      "|125535470| 5.0|Aviva is such a n...|               en|Aviva is such a n...|\n",
      "+---------+----+--------------------+-----------------+--------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "spark_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-08T08:52:06.666365Z",
     "iopub.status.busy": "2024-05-08T08:52:06.665408Z",
     "iopub.status.idle": "2024-05-08T08:52:06.690776Z",
     "shell.execute_reply": "2024-05-08T08:52:06.688904Z",
     "shell.execute_reply.started": "2024-05-08T08:52:06.666155Z"
    }
   },
   "outputs": [],
   "source": [
    "# spark_df.write.csv(\"hdfs://m1:8020/output2.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-08T08:52:06.698415Z",
     "iopub.status.busy": "2024-05-08T08:52:06.695703Z",
     "iopub.status.idle": "2024-05-08T08:52:06.850811Z",
     "shell.execute_reply": "2024-05-08T08:52:06.850094Z",
     "shell.execute_reply.started": "2024-05-08T08:52:06.698366Z"
    },
    "id": "7U8185MvU5Kn"
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import when\n",
    "# Set sentiment based on rate\n",
    "spark_df = spark_df.withColumn('sentiment',\n",
    "                   when(spark_df['rate'] < 2, 'negative')\n",
    "                   .when((spark_df['rate'] >= 2) & (spark_df['rate'] <= 3), 'neutral')\n",
    "                   .otherwise('positive'))\n",
    "spark_df = spark_df.withColumn('label',\n",
    "                   when(spark_df['rate'] < 2, 0)\n",
    "                   .when((spark_df['rate'] >= 2) & (spark_df['rate'] <= 3), 50)\n",
    "                   .otherwise(99))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2024-05-08T08:52:06.856865Z",
     "iopub.status.busy": "2024-05-08T08:52:06.855952Z",
     "iopub.status.idle": "2024-05-08T08:52:08.906334Z",
     "shell.execute_reply": "2024-05-08T08:52:08.900633Z",
     "shell.execute_reply.started": "2024-05-08T08:52:06.856829Z"
    },
    "id": "6Ul-ysebw8V-",
    "outputId": "a89f2c90-95a0-4390-83aa-a026b5686d5e"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 66:===============================================>        (27 + 4) / 32]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----+\n",
      "|sentiment|count|\n",
      "+---------+-----+\n",
      "| negative|    2|\n",
      "|  neutral|    2|\n",
      "| positive|    3|\n",
      "+---------+-----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "spark_df.groupBy(\"sentiment\").count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-08T08:52:08.910231Z",
     "iopub.status.busy": "2024-05-08T08:52:08.909717Z",
     "iopub.status.idle": "2024-05-08T08:52:09.747576Z",
     "shell.execute_reply": "2024-05-08T08:52:09.745941Z",
     "shell.execute_reply.started": "2024-05-08T08:52:08.910149Z"
    },
    "id": "nHShoeSVVB-s"
   },
   "outputs": [],
   "source": [
    "#  convert emoji and emoticon into words\n",
    "emot_obj = emot.core.emot()\n",
    "def replace_emojis_with_meanings(text):\n",
    "    emojis = emot_obj.emoji(text)['value']\n",
    "    emojis_meanings =  emot_obj.emoji(text)['mean']\n",
    "    emoticons = emot_obj.emoticons(text)['value']\n",
    "    emoticons_meanings =  emot_obj.emoticons(text)['mean']\n",
    "\n",
    "    # Remove special characters from meanings list\n",
    "    emojis_meanings = [re.sub(r'[^\\w\\s]', '', meaning) for meaning in emojis_meanings]\n",
    "     # Remove special characters from meanings list\n",
    "    emoticons_meanings = [re.sub(r'[^\\w\\s]', '', meaning) for meaning in emoticons_meanings]\n",
    "\n",
    "    # Replace emojis in the text with the corresponding meanings\n",
    "    for emoji, meaning in zip(emojis, emojis_meanings):\n",
    "        text = text.replace(emoji, meaning)\n",
    "\n",
    "     # Replace emojis in the text with the corresponding meanings\n",
    "    for emoji, meaning in zip(emoticons, emoticons_meanings):\n",
    "        text = text.replace(emoji, meaning)\n",
    "\n",
    "    return text\n",
    "replace_emojis_with_meanings_udf = udf(replace_emojis_with_meanings, StringType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-08T08:52:09.750286Z",
     "iopub.status.busy": "2024-05-08T08:52:09.748920Z",
     "iopub.status.idle": "2024-05-08T08:52:09.758708Z",
     "shell.execute_reply": "2024-05-08T08:52:09.755622Z",
     "shell.execute_reply.started": "2024-05-08T08:52:09.750246Z"
    },
    "id": "a5mH63pWVEG2"
   },
   "outputs": [],
   "source": [
    "# Function to remove links using regular expressions\n",
    "def remove_links(comment):\n",
    "    # Regular expression pattern to match URLs\n",
    "    pattern = r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\\\(\\\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+'\n",
    "    return re.sub(pattern, '', comment)\n",
    "remove_links_udf = udf(remove_links, StringType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-08T08:52:09.764526Z",
     "iopub.status.busy": "2024-05-08T08:52:09.762218Z",
     "iopub.status.idle": "2024-05-08T08:52:09.775100Z",
     "shell.execute_reply": "2024-05-08T08:52:09.772237Z",
     "shell.execute_reply.started": "2024-05-08T08:52:09.763780Z"
    },
    "id": "1-lF-J-QWBfE"
   },
   "outputs": [],
   "source": [
    "# Function to remove HTML tags\n",
    "def remove_html_tags(text):\n",
    "    soup = BeautifulSoup(text, \"html.parser\")\n",
    "    # Get the text without HTML tags\n",
    "    text_without_tags = soup.get_text(separator=\" \", strip=True)\n",
    "    return text_without_tags\n",
    "remove_html_tags_udf = udf(remove_html_tags, StringType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-08T08:52:09.779004Z",
     "iopub.status.busy": "2024-05-08T08:52:09.777984Z",
     "iopub.status.idle": "2024-05-08T08:52:09.928827Z",
     "shell.execute_reply": "2024-05-08T08:52:09.927311Z",
     "shell.execute_reply.started": "2024-05-08T08:52:09.778965Z"
    },
    "id": "y3zxaF6PWIrh"
   },
   "outputs": [],
   "source": [
    "# Function to remove_numbers_or_special_chars\n",
    "def remove_numbers_or_special_chars(comment):\n",
    "     # Replace escape sequences with an empty string\n",
    "    cleaned_text = re.sub(r'\\\\[^\\s]', ' ', comment)\n",
    "    # convert n't into not EX(didn't => did not)\n",
    "    cleaned_text= cleaned_text.replace(\"n't\", \" not\")\n",
    "    cleaned_text= cleaned_text.replace(\"_\", \" \")\n",
    "    # Remove special characters and spaces\n",
    "    cleaned_text = re.sub(r'\\W+', ' ',cleaned_text)\n",
    "    # Remove special numbers\n",
    "    cleaned_text =re.sub(r'\\d+', ' ', cleaned_text)\n",
    "    return cleaned_text\n",
    "remove_numbers_or_special_chars_udf = udf(remove_html_tags, StringType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-08T08:52:09.930864Z",
     "iopub.status.busy": "2024-05-08T08:52:09.930451Z",
     "iopub.status.idle": "2024-05-08T08:52:09.957956Z",
     "shell.execute_reply": "2024-05-08T08:52:09.956238Z",
     "shell.execute_reply.started": "2024-05-08T08:52:09.930833Z"
    },
    "id": "DxpmLtCyWPsa"
   },
   "outputs": [],
   "source": [
    "# apply cleaning functions on comments\n",
    "def clean_text(comment):\n",
    "    text_res = replace_emojis_with_meanings(comment)\n",
    "    text_res = text_res.lower()\n",
    "    text_res = remove_links(text_res)\n",
    "    text_res = remove_html_tags(text_res)\n",
    "    text_res = remove_numbers_or_special_chars(text_res)\n",
    "    return text_res\n",
    "clean_text_udf = udf(clean_text, StringType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2024-05-08T08:52:09.961849Z",
     "iopub.status.busy": "2024-05-08T08:52:09.961089Z",
     "iopub.status.idle": "2024-05-08T08:52:10.695656Z",
     "shell.execute_reply": "2024-05-08T08:52:10.694298Z",
     "shell.execute_reply.started": "2024-05-08T08:52:09.961803Z"
    },
    "id": "v7L00kDaxVdH",
    "outputId": "1769d059-eb1a-4517-fcb4-9bc61424662f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----+--------------------+-----------------+--------------------+---------+-----+\n",
      "|       id|rate|             comment|detected_language|     cleaned_comment|sentiment|label|\n",
      "+---------+----+--------------------+-----------------+--------------------+---------+-----+\n",
      "|124129784| 1.0|nothing informati...|               en|nothing informati...| negative|    0|\n",
      "| 88962892| 1.0|I think a beginne...|               en|I think a beginne...| negative|    0|\n",
      "|121769970| 3.5|Multiple spelling...|               en|Multiple spelling...|  neutral|   50|\n",
      "| 76584052| 4.5|I found this cour...|               en|I found this cour...| positive|   99|\n",
      "|125029758| 5.0|This course is th...|               en|This course is th...| positive|   99|\n",
      "| 68767147| 3.5|Muy buena la intr...|               es|The introduction ...|  neutral|   50|\n",
      "|125535470| 5.0|Aviva is such a n...|               en|Aviva is such a n...| positive|   99|\n",
      "+---------+----+--------------------+-----------------+--------------------+---------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-08T08:52:10.697621Z",
     "iopub.status.busy": "2024-05-08T08:52:10.697090Z",
     "iopub.status.idle": "2024-05-08T08:52:10.788187Z",
     "shell.execute_reply": "2024-05-08T08:52:10.787335Z",
     "shell.execute_reply.started": "2024-05-08T08:52:10.697577Z"
    },
    "id": "D26aKgH9WXdP"
   },
   "outputs": [],
   "source": [
    "# Apply the clean_text function to the 'cleaned_comment' column\n",
    "spark_df = spark_df.withColumn('cleaned_comment', clean_text_udf(spark_df['cleaned_comment']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-08T08:52:10.795935Z",
     "iopub.status.busy": "2024-05-08T08:52:10.794745Z",
     "iopub.status.idle": "2024-05-08T08:52:10.834831Z",
     "shell.execute_reply": "2024-05-08T08:52:10.833516Z",
     "shell.execute_reply.started": "2024-05-08T08:52:10.795898Z"
    },
    "id": "66jtPwG1XTOH"
   },
   "outputs": [],
   "source": [
    "# remove stop words and lemmatization\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "# Words to keep\n",
    "\n",
    "words_to_keep = {'very','too','so','not','no','but'}\n",
    "# keep words like \"very\", and \"so\"\n",
    "stop_words = {word for word in stop_words if  word not in words_to_keep}\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def preprocess_text(comment):\n",
    "      tokens = word_tokenize(comment.lower())  # Tokenization\n",
    "      tokens = [token for token in tokens if token.isalpha()]  # Remove non-alphabetic tokens\n",
    "      tokens = [token for token in tokens if token not in stop_words]  # Remove stop words\n",
    "      tokens = [lemmatizer.lemmatize(token) for token in tokens]  # Lemmatization\n",
    "      return ' '.join(tokens)\n",
    "preprocess_text_udf = udf(preprocess_text, StringType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-08T08:52:10.836987Z",
     "iopub.status.busy": "2024-05-08T08:52:10.836525Z",
     "iopub.status.idle": "2024-05-08T08:52:10.997724Z",
     "shell.execute_reply": "2024-05-08T08:52:10.989101Z",
     "shell.execute_reply.started": "2024-05-08T08:52:10.836953Z"
    },
    "id": "7MHQ0RcwXVl9"
   },
   "outputs": [],
   "source": [
    "spark_df = spark_df.withColumn('cleaned_text', preprocess_text_udf(spark_df['cleaned_comment']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2024-05-08T08:52:10.999828Z",
     "iopub.status.busy": "2024-05-08T08:52:10.999288Z",
     "iopub.status.idle": "2024-05-08T08:52:31.505350Z",
     "shell.execute_reply": "2024-05-08T08:52:31.498772Z",
     "shell.execute_reply.started": "2024-05-08T08:52:10.999790Z"
    },
    "id": "EyZP2VjBv-Qo",
    "outputId": "091300be-d3b4-4b70-ae54-7825fb2e8d29"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----+--------------------+-----------------+--------------------+---------+-----+--------------------+\n",
      "|       id|rate|             comment|detected_language|     cleaned_comment|sentiment|label|        cleaned_text|\n",
      "+---------+----+--------------------+-----------------+--------------------+---------+-----+--------------------+\n",
      "|124129784| 1.0|nothing informati...|               en|nothing informati...| negative|    0| nothing information|\n",
      "| 88962892| 1.0|I think a beginne...|               en|i think a beginne...| negative|    0|think beginner ne...|\n",
      "|121769970| 3.5|Multiple spelling...|               en|multiple spelling...|  neutral|   50|multiple spelling...|\n",
      "| 76584052| 4.5|I found this cour...|               en|i found this cour...| positive|   99|found course very...|\n",
      "|125029758| 5.0|This course is th...|               en|this course is th...| positive|   99|course best udemy...|\n",
      "| 68767147| 3.5|Muy buena la intr...|               es|the introduction ...|  neutral|   50|introduction very...|\n",
      "|125535470| 5.0|Aviva is such a n...|               en|aviva is such a n...| positive|   99|aviva natural tea...|\n",
      "+---------+----+--------------------+-----------------+--------------------+---------+-----+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-08T08:52:31.552305Z",
     "iopub.status.busy": "2024-05-08T08:52:31.524497Z",
     "iopub.status.idle": "2024-05-08T08:52:32.900258Z",
     "shell.execute_reply": "2024-05-08T08:52:32.768335Z",
     "shell.execute_reply.started": "2024-05-08T08:52:31.552200Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['id',\n",
       " 'rate',\n",
       " 'comment',\n",
       " 'detected_language',\n",
       " 'cleaned_comment',\n",
       " 'sentiment',\n",
       " 'label',\n",
       " 'cleaned_text']"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-08T08:55:54.179244Z",
     "iopub.status.busy": "2024-05-08T08:55:54.178723Z",
     "iopub.status.idle": "2024-05-08T08:55:58.554708Z",
     "shell.execute_reply": "2024-05-08T08:55:58.552173Z",
     "shell.execute_reply.started": "2024-05-08T08:55:54.179208Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from couchdb import Server\n",
    "\n",
    "\n",
    "# Create a CouchDB Server object\n",
    "server = Server(\"http://172.26.0.5:5984/\")\n",
    "\n",
    "# Authenticate with CouchDB (if required)\n",
    "server.resource.credentials = (\"admin\", \"admin\")\n",
    "\n",
    "# Create or get the database\n",
    "db_name = \"nlp_db\"\n",
    "db = server.create(db_name)\n",
    "\n",
    "# Loop over each row in the DataFrame and save to CouchDB\n",
    "for row in spark_df.collect():\n",
    "    doc = {\n",
    "        \"_id\": row.id,\n",
    "        \"rate\": row.rate,\n",
    "        \"comment\": row.comment,\n",
    "        \"detected_language\": row.detected_language,\n",
    "        \"cleaned_comment\": row.cleaned_comment,\n",
    "        \"sentiment\": row.sentiment,\n",
    "        \"label\": row.label,\n",
    "        \"cleaned_text\": row.cleaned_text\n",
    "    }\n",
    "    db.save(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.status.busy": "2024-05-08T08:52:44.588127Z",
     "iopub.status.idle": "2024-05-08T08:52:44.588751Z",
     "shell.execute_reply": "2024-05-08T08:52:44.588599Z",
     "shell.execute_reply.started": "2024-05-08T08:52:44.588585Z"
    },
    "id": "bsT89PtnFm1d",
    "outputId": "cf6092f6-a25a-4af7-86ab-d48e6fa3ef6e"
   },
   "outputs": [],
   "source": [
    "# # Define a dictionary to map sentiment values to label values\n",
    "# sentiment_label_map = {\"positive\": 1, \"neutral\": 0, \"negative\": -1}\n",
    "\n",
    "# # Loop over sentiment values and set new column called 'label'\n",
    "# for sentiment, label in sentiment_label_map.items():\n",
    "#     spark_df = spark_df.withColumn(\"label\", when(spark_df[\"sentiment\"] == sentiment, label))\n",
    "# spark_df.show(spark_df.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-05-08T08:52:44.590188Z",
     "iopub.status.idle": "2024-05-08T08:52:44.590934Z",
     "shell.execute_reply": "2024-05-08T08:52:44.590732Z",
     "shell.execute_reply.started": "2024-05-08T08:52:44.590711Z"
    },
    "id": "qLiSB4_Dzy9S"
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import Tokenizer, StopWordsRemover, CountVectorizer, IDF, StringIndexer\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml import Pipeline\n",
    "# Sample data\n",
    "# Replace `spark_df` with your actual Spark DataFrame containing 'cleaned_text' and 'sentiment' columns\n",
    "# Assuming 'sentiment' is your target column and 'cleaned_text' is your feature column\n",
    "# Replace `test_ratio` with your desired test set ratio\n",
    "test_ratio = 0.2\n",
    "\n",
    "# Split the data into train and test sets\n",
    "train_data, test_data = spark_df.randomSplit([1 - test_ratio, test_ratio], seed=42)\n",
    "\n",
    "# Define the stages of the pipeline\n",
    "tokenizer = Tokenizer(inputCol=\"cleaned_text\", outputCol=\"words\")\n",
    "count_vectorizer = CountVectorizer(inputCol=\"words\", outputCol=\"rawFeatures\")\n",
    "idf = IDF(inputCol=\"rawFeatures\", outputCol=\"features\")\n",
    "# Define the classifier model\n",
    "rf = RandomForestClassifier(featuresCol=\"features\", labelCol=\"label\", seed=42)\n",
    "\n",
    "# Define the pipeline\n",
    "pipeline = Pipeline(stages=[ tokenizer,count_vectorizer, idf, rf])\n",
    "\n",
    "# Train the model\n",
    "model = pipeline.fit(train_data)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.status.busy": "2024-05-08T08:52:44.593222Z",
     "iopub.status.idle": "2024-05-08T08:52:44.594617Z",
     "shell.execute_reply": "2024-05-08T08:52:44.593832Z",
     "shell.execute_reply.started": "2024-05-08T08:52:44.593811Z"
    },
    "id": "J5IVo6H4Ca9J",
    "outputId": "55fff348-8466-4ca0-b3be-426829f5c8eb"
   },
   "outputs": [],
   "source": [
    "# Make predictions on the test data\n",
    "predictions = model.transform(test_data)\n",
    "\n",
    "# Show some sample predictions\n",
    "predictions.select(\"cleaned_text\", \"sentiment\", \"prediction\").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1TtmHRJoCvqo",
    "outputId": "d052e54e-2df7-44b6-b1b9-3d605c191904"
   },
   "outputs": [],
   "source": [
    "# prompt: with my model make prodection for this comment \"i really hate that !\"\n",
    "\n",
    "comment = \"i really love that !\"\n",
    "cleaned_comment = clean_text(comment)\n",
    "cleaned_text = preprocess_text(cleaned_comment)\n",
    "data = sc.createDataFrame([(cleaned_text, )], [\"cleaned_text\"])\n",
    "predictions = model.transform(data)\n",
    "predicted_sentiment = predictions.select(\"prediction\").collect()[0][0]\n",
    "print(f\"Predicted sentiment: {predicted_sentiment}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ey3cVNicA_0G"
   },
   "outputs": [],
   "source": [
    "# # Evaluate the model (example: using MulticlassClassificationEvaluator)\n",
    "# from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "# evaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "# accuracy = evaluator.evaluate(predictions)\n",
    "# print(\"Test Accuracy =\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import HashingTF, IDF, Tokenizer\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.ml.pipeline import Pipeline\n",
    "\n",
    "## Load the data into a Spark DataFrame\n",
    "df = spark_df[[\"cleaned_text\", \"label\"]]\n",
    "\n",
    "## Split the data into training and testing sets\n",
    "(trainDF, testDF) = df.randomSplit([0.8, 0.2], seed=42)\n",
    "\n",
    "## Define a pipeline with Tokenizer, HashingTF, IDF, and Logistic Regression\n",
    "tokenizer = Tokenizer(inputCol=\"cleaned_text\", outputCol=\"words\")\n",
    "hashingTF = HashingTF(inputCol=\"words\", outputCol=\"features\", numFeatures=5000)\n",
    "idf = IDF(inputCol=\"features\", outputCol=\"tfidf_features\")\n",
    "lr = LogisticRegression(featuresCol=\"tfidf_features\", labelCol=\"label\")\n",
    "\n",
    "pipeline = Pipeline(stages=[tokenizer, hashingTF, idf, lr])\n",
    "\n",
    "## Train the model\n",
    "model = pipeline.fit(trainDF)\n",
    "\n",
    "## Make predictions\n",
    "trainPredictions = model.transform(trainDF)\n",
    "testPredictions = model.transform(testDF)\n",
    "\n",
    "## Calculate accuracy\n",
    "trainAccuracy = MulticlassClassificationEvaluator(labelCol=\"lable\", predictionCol=\"prediction\", metricName=\"accuracy\").evaluate(trainPredictions)\n",
    "testAccuracy = MulticlassClassificationEvaluator(labelCol=\"lable\", predictionCol=\"prediction\", metricName=\"accuracy\").evaluate(testPredictions)\n",
    "\n",
    "print(f\"Training Accuracy: {trainAccuracy:.4f}\")\n",
    "print(f\"Testing Accuracy: {testAccuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import HashingTF, IDF, Tokenizer\n",
    "from pyspark.ml.classification import NaiveBayes\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.ml.pipeline import Pipeline\n",
    "\n",
    "## Load the data into a Spark DataFrame\n",
    "\n",
    "\n",
    "## Define a pipeline with Tokenizer, HashingTF, IDF, and Multinomial Naive Bayes\n",
    "tokenizer = Tokenizer(inputCol=\"cleaned_text\", outputCol=\"words\")\n",
    "hashingTF = HashingTF(inputCol=\"words\", outputCol=\"features\", numFeatures=5000)\n",
    "idf = IDF(inputCol=\"features\", outputCol=\"tfidf_features\")\n",
    "nb = NaiveBayes(featuresCol=\"tfidf_features\", labelCol=\"label\")\n",
    "\n",
    "pipeline = Pipeline(stages=[tokenizer, hashingTF, idf, nb])\n",
    "\n",
    "## Train the model\n",
    "model = pipeline.fit(trainDF)\n",
    "\n",
    "## Make predictions\n",
    "trainPredictions = model.transform(trainDF)\n",
    "testPredictions = model.transform(testDF)\n",
    "\n",
    "## Calculate accuracy\n",
    "trainAccuracy = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\").evaluate(trainPredictions)\n",
    "testAccuracy = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\").evaluate(testPredictions)\n",
    "\n",
    "print(f\"Multinomial Naive Bayes Training Accuracy: {trainAccuracy:.4f}\")\n",
    "print(f\"Multinomial Naive Bayes Testing Accuracy: {testAccuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import HashingTF, IDF, Tokenizer\n",
    "from pyspark.ml.classification import SVM\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.ml.pipeline import Pipeline\n",
    "\n",
    "\n",
    "## Define a pipeline with Tokenizer, HashingTF, IDF, and Support Vector Machine\n",
    "tokenizer = Tokenizer(inputCol=\"clean_text\", outputCol=\"words\")\n",
    "hashingTF = HashingTF(inputCol=\"words\", outputCol=\"features\", numFeatures=5000)\n",
    "idf = IDF(inputCol=\"features\", outputCol=\"tfidf_features\")\n",
    "svm = SVM(featuresCol=\"tfidf_features\", labelCol=\"sentiment\", kernel=\"linear\", C=1.0)\n",
    "\n",
    "pipeline = Pipeline(stages=[tokenizer, hashingTF, idf, svm])\n",
    "\n",
    "## Train the model\n",
    "model = pipeline.fit(trainDF)\n",
    "\n",
    "## Make predictions\n",
    "trainPredictions = model.transform(trainDF)\n",
    "testPredictions = model.transform(testDF)\n",
    "\n",
    "## Calculate accuracy\n",
    "trainAccuracy = MulticlassClassificationEvaluator(labelCol=\"sentiment\", predictionCol=\"prediction\", metricName=\"accuracy\").evaluate(trainPredictions)\n",
    "testAccuracy = MulticlassClassificationEvaluator(labelCol=\"sentiment\", predictionCol=\"prediction\", metricName=\"accuracy\").evaluate(testPredictions)\n",
    "\n",
    "print(f\"Accuracy: {testAccuracy:.2f}\")\n",
    "\n",
    "## Generate classification report\n",
    "report = MulticlassClassificationEvaluator(labelCol=\"sentiment\", predictionCol=\"prediction\", metricName=\"f1\").evaluate(testPredictions)\n",
    "print(\"Classification Report:\")\n",
    "print(f\"F1 Score: {report:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
